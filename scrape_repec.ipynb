{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup, ResultSet\n",
    "import urllib.request\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(666)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677be535c3a4470ea63839b744daae0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Connect to SQL Database and reads every Table in the whole DB\n",
    "# Then converts each table into a .csv file so we can work with themn in Python\n",
    "\n",
    "def to_csv():\n",
    "\n",
    "    os.chdir(\"/home/chris/HOPE/\") # HARDCODED \n",
    "    db = sqlite3.connect('read.db')\n",
    "    os.chdir(\"/home/chris/HOPE/old_db\") # HARDCODED \n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    for table_name in tqdm_notebook(tables):\n",
    "        table_name = table_name[0]\n",
    "        table = pd.read_sql_query(\"SELECT * from %s\" % table_name, db)\n",
    "        table.to_csv(table_name + '.csv', index_label='index')\n",
    "    cursor.close()\n",
    "    db.close()\n",
    "\n",
    "to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     0\n",
      "0      AuthorAlias.csv\n",
      "0              JEL.csv\n",
      "0         Children.csv\n",
      "0             Nber.csv\n",
      "0         NBERCorr.csv\n",
      "0         NBERStat.csv\n",
      "0  sqlite_sequence.csv\n",
      "0             Inst.csv\n",
      "0     sqlite_stat4.csv\n",
      "0         InstCorr.csv\n",
      "0        InstAlias.csv\n",
      "0      JournalName.csv\n",
      "0          Article.csv\n",
      "0         ReadStat.csv\n",
      "0     sqlite_stat1.csv\n",
      "0       AuthorCorr.csv\n",
      "0          DOICorr.csv\n",
      "0           Author.csv\n",
      "0      EditorBoard.csv\n"
     ]
    }
   ],
   "source": [
    "# Print the names of the 17 individual .db tables you just built\n",
    "\n",
    "files = pd.DataFrame([])\n",
    "for fname in glob.glob(\"/home/chris/HOPE/old_db/*.csv\"): # HARDCODED \n",
    "    fname = fname.rsplit(\"/\", 1)\n",
    "    fname = pd.DataFrame(pd.Series(fname[1]))\n",
    "    files = files.append(fname)\n",
    "print(files)\n",
    "\n",
    "\n",
    "# Load the data you just found (technical debt?)\n",
    "\n",
    "AuthorAlias =      pd.read_csv(\"AuthorAlias.csv\")\n",
    "JEL =              pd.read_csv(\"JEL.csv\")\n",
    "Children =         pd.read_csv(\"Children.csv\")\n",
    "Nber =             pd.read_csv(\"Nber.csv\")\n",
    "NBERCorr =         pd.read_csv(\"NBERCorr.csv\")\n",
    "NBERStat =         pd.read_csv(\"NBERStat.csv\")\n",
    "sqlite_sequence =  pd.read_csv(\"sqlite_sequence.csv\")\n",
    "Inst =             pd.read_csv(\"Inst.csv\")\n",
    "sqlite_stat4 =     pd.read_csv(\"sqlite_stat4.csv\")\n",
    "InstCorr =         pd.read_csv(\"InstCorr.csv\")\n",
    "InstAlias =        pd.read_csv(\"InstAlias.csv\")\n",
    "JournalName =      pd.read_csv(\"JournalName.csv\")\n",
    "Article =          pd.read_csv(\"Article.csv\")\n",
    "ReadStat =         pd.read_csv(\"ReadStat.csv\")\n",
    "sqlite_stat1 =     pd.read_csv(\"sqlite_stat1.csv\")\n",
    "AuthorCorr =       pd.read_csv(\"AuthorCorr.csv\")\n",
    "DOICorr =          pd.read_csv(\"DOICorr.csv\")\n",
    "Author =           pd.read_csv(\"Author.csv\")\n",
    "EditorBoard =      pd.read_csv(\"EditorBoard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_latest(journal):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function finds the \"newest\" metadata and saves it as variables that we use later on\n",
    "    We then use these variables later on to automate thresholds and cutoffs\n",
    "    This is done bcause we specficially do NOT WANT to grab any data prior to our cutoff\n",
    "    \"\"\"\n",
    "    \n",
    "    data = Article[Article[\"Journal\"] == journal]\n",
    "    data = data.sort_values(by=[\"Volume\", \"Issue\"])\n",
    "    data = pd.DataFrame(data.iloc[-1,:])\n",
    "    data = data.T\n",
    "    volume = int(data[\"Volume\"])\n",
    "    issue = int(data[\"Issue\"])\n",
    "\n",
    "    return(volume, issue)\n",
    "\n",
    "latest_eca_volume, latest_eca_issue = find_latest(\"ECA\")\n",
    "latest_aer_volume, latest_aer_issue = find_latest(\"AER\")\n",
    "latest_jpe_volume, latest_jpe_issue = find_latest(\"JPE\")\n",
    "latest_qje_volume, latest_qje_issue = find_latest(\"QJE\")\n",
    "latest_res_volume, latest_res_issue = find_latest(\"RES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_repec(journal_name, root_url, pages, latest_volume, latest_issue):\n",
    "\n",
    "    \"\"\"\n",
    "    This is a webcrawler that crawls through every page/article of a\n",
    "    TO DO: Make the webcrawler  \"smarter\" so that it can detect that there are 17 pages of data to scrape\n",
    "    TO DO: Scrape more than 5 journals\n",
    "    \"\"\"\n",
    "\n",
    "    # Soupa nd link \n",
    "    links = pd.DataFrame([])\n",
    "    for i in tqdm_notebook(range(1, pages)):\n",
    "        url = root_url       \n",
    "        if i > 1:  # \"1\" workaround is due to their uniue naming convenitons\n",
    "            url = url[:-5] + str(i) + url[-5:]\n",
    "        html_page = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "        for link in soup.find_all('a', href=True): # Grab all the URL hyperlinks on the webpage\n",
    "            links = links.append(pd.DataFrame(pd.Series(link['href']))) \n",
    "    links.columns = [\"url\"]\n",
    "    links = links[links['url'].str.contains(\"/a/\")]\n",
    "    links = \"https://ideas.repec.org\" + links\n",
    "    links[\"metadata\"] = links[\"url\"]\n",
    "\n",
    "    # Metadata manipulation; haervests volume, issue, year, and pages of the scrape d ata\n",
    "    # TECHNICAL DEBT! These 6 lines are ugly and gross\n",
    "    links[\"metadata\"] = links[\"metadata\"].str.split(\"v\", n=1, expand=True)[1]\n",
    "    links[\"Volume\"] = links[\"metadata\"].str.split(\"y\", n=1, expand=True)[0]\n",
    "    links[\"metadata\"] = links[\"metadata\"].str.split(\"y\", n=1, expand=True)[1]\n",
    "    links[\"y\"] = links[\"metadata\"].str.split(\"i\", n=1, expand=True)[0]\n",
    "    links[\"metadata\"] = links[\"metadata\"].str.split(\"i\", n=1, expand=True)[1]\n",
    "    links[\"Issue\"] = links[\"metadata\"].str.split(\"p\", n=1, expand=True)[0]\n",
    "    links[\"metadata\"] = links[\"metadata\"].str.split(\"p\", n=1, expand=True)[1]\n",
    "    links[\"FirstPage\"] = links[\"metadata\"].str.split(\"-\", n=1, expand=True)[0]\n",
    "    links[\"metadata\"] = links[\"metadata\"].str.split(\"-\", n=1, expand=True)[1]\n",
    "    links[\"LastPage\"] = links[\"metadata\"].str.split(\".\", n=1, expand=True)[0]\n",
    "    links = links.drop(columns = [\"metadata\"])\n",
    "    links[\"Volume\"] = pd.to_numeric(links[\"Volume\"])\n",
    "    links = links[links[\"Volume\"] > latest_volume] # CUTOFF FOR NEWEST ONLY \n",
    "    \n",
    "    # Now we loop through all the individual URLs to scrape some more specific metadata\n",
    "    # Question: Do citations from the original data soruce hold up?\n",
    "    data = pd.DataFrame([]) # intialize empty dfs to be filled up in loop\n",
    "    authors = pd.DataFrame([])\n",
    "    for url in tqdm_notebook(links[\"url\"]):\n",
    "        df = links[links[\"url\"] == url]\n",
    "        html_page = urllib.request.urlopen(url)\n",
    "        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "        \n",
    "        # Author(s)        \n",
    "        author = str(soup.find(id =\"author\"))\n",
    "        author = pd.DataFrame(pd.Series(str(author[1:-1]))) # HARDCODED \n",
    "        # Question - how to get institution?\n",
    "        # Question - Institution \n",
    "        \n",
    "        # Title\n",
    "        title = str(soup.find(id =\"title\"))\n",
    "        title = str(title[21:-11]) # HARDCODED \n",
    "        df[\"Title\"] = title\n",
    "\n",
    "        # Abstract        \n",
    "        abstract = str(soup.find(id =\"abstract-body\"))\n",
    "        abstract = str(abstract[24:-6]) # HARDCODED \n",
    "        df[\"Abstract\"] = abstract\n",
    "\n",
    "        # Citations\n",
    "        citations = str(soup.find(id =\"cites-tab\"))\n",
    "        if len(citations) == 4:\n",
    "            df[\"CiteCount\"] = 0\n",
    "        else:\n",
    "            citations = str(citations.split(\"role\")[1])\n",
    "            citations = str(citations.split(\"Citations\")[0])\n",
    "            citations = str(citations.split(\">\")[1])\n",
    "            df[\"CiteCount\"] = citations\n",
    "        \n",
    "        # Create empty columns (quick/dirty workaround) that formats table such that you can append it to SQL \n",
    "        df[\"Journal\"] = journal_name\n",
    "        df[\"index\"] = \"MISSING\"\n",
    "        df[\"ArticleID\"] = \"MISSING\"\n",
    "        df[\"Language\"] = \"MISSING\"\n",
    "        df[\"Received\"] = \"MISSING\"\n",
    "        df[\"Accepted\"] = \"MISSING\"\n",
    "        df[\"Part\"] = \"MISSING\"\n",
    "        df[\"Comments\"] = \"MISSING\"\n",
    "        df[\"Note\"] = \"MISSING\"\n",
    "        df[\"PubDate\"] = \"MISSING\"\n",
    "        df = df.drop(columns = [\"y\", \"url\"]) #  Technical debt \n",
    "        \n",
    "        # Append!\n",
    "        data = data.append(df) \n",
    "        authors = authors.append(author)\n",
    "    data = data[Article.columns] # HARDCODED \n",
    "    data = pd.DataFrame(qje)\n",
    "    \n",
    "    return(data, authors)\n",
    "\n",
    "# TOMROROW: FInish all 17 tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5016486e3124d4eb754c532725787a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1083c7b47dd4034bf49be3cf672b8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=173.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "qje = scrape_repec(journal_name = \"QJE\", root_url = \"https://ideas.repec.org/s/oup/qjecon.html\", pages = 10, latest_volume = latest_qje_volume, latest_issue = latest_qje_issue) \n",
    "\n",
    "os.chdir(\"/home/chris/HOPE/new_db\") # HARDCODED \n",
    "qje.to_csv(\"Article.csv\")\n",
    "\n",
    "#aer = scrape_article_urls(journal_name = \"AER\",root_url = \"https://ideas.repec.org/s/aea/aecrev.html\", pages = 10, latest_volume = latest_aer_volume, latest_issue = latest_aer_issue)\n",
    "#jpe = scrape_article_urls(journal_name = \"JPE\",root_url = \"https://ideas.repec.org/s/ucp/jpolec.html\", pages = 10, latest_volume = latest_jpe_volume, latest_issue = latest_jpe_issue)\n",
    "#eca = scrape_article_urls(journal_name = \"ECA\",root_url = \"https://ideas.repec.org/s/ecm/emetrp.html\", pages = 10, latest_volume = latest_eca_volume, latest_issue = latest_eca_issue)\n",
    "#res = scrape_article_urls(journal_name = \"RES\",root_url = \"https://ideas.repec.org/s/oup/restud.html.html\", pages = 10, latest_jres_volume = 2, latest_issue = latest_res_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Junk / Under-construction\n",
    "\n",
    "#html_page = urllib.request.urlopen(\"https://ideas.repec.org/a/oup/qjecon/v125y2010i2p729-765..html\")\n",
    "#soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "#        # Author(s)        \n",
    "#author = str(soup.find(id =\"listed-authors\"))\n",
    "#print(author)\n",
    "#author = pd.DataFrame(pd.Series(str(author[1:-1]))) # HARDCODED \n",
    "#authorr = pd.DataFrame(pd.Series(str(author.split(\"authorname\",1))))\n",
    "#authorr.iloc[0,0] = authorr.iloc[0,0][82:-20] #HARDCODED (gross)\n",
    "#\n",
    "#print(authorr.iloc[0,0])\n",
    "#\n",
    "#    print(authorr[i])\n",
    "\n",
    "\n",
    "\n",
    "# JEL Codes per Journal - how have they evolved over time?\n",
    "# Technichal debt - can be made much sleeker\n",
    "#set(JournalName.Journal)\n",
    "#ECA_articles = set(Article[Article[\"Journal\"] == \"ECA\"].iloc[:,1])\n",
    "#AER_articles = set(Article[Article[\"Journal\"] == \"AER\"].iloc[:,1])\n",
    "#JPE_articles = set(Article[Article[\"Journal\"] == \"JPE\"].iloc[:,1])\n",
    "#RES_articles = set(Article[Article[\"Journal\"] == \"RES\"].iloc[:,1])\n",
    "#QJE_articles = set(Article[Article[\"Journal\"] == \"QJE\"].iloc[:,1])\n",
    "#top5_articles = [ECA_articles, AER_articles, JPE_articles, RES_articles, QJE_articles]\n",
    "\n",
    "# Make flags for every JEL code via loop\n",
    "# I figure we might need these flags for fixed effects or something at some point\n",
    "#df = Article # I just like working with df better\n",
    "#articles = set(Article.ArticleID)\n",
    "#JEL_codes =\n",
    "#for article in tqdm(articles):\n",
    "\n",
    "# Now split this graph into two\n",
    "#    chunk = JEL[JEL[\"ArticleID\"] == article]\n",
    "#    moving average of JEL codes per year per journal\n",
    "#\n",
    "#    print(df)\n",
    "#    print(article)\n",
    "\n",
    "# That's a picture for the entier thign, yes but how have they evolved over time?\n",
    "# 1 Pie Chart Per Decade Per Journal of JEL Codes\n",
    "# Investigate articles with zero JEL codes. Is that something I have to go get?\n",
    "\n",
    "#JEL\n",
    "#JEL_tally = JEL.groupby(\"JEL\").count()\n",
    "#JEL_tally = JEL_tally.iloc[:,1]\n",
    "#print( JEL_tally )\n",
    "\n",
    "# The growth of the averga page length over time\n",
    "\n",
    "# Difference in page length between men and women\n",
    "\n",
    "# Exploratory regressions\n",
    "\n",
    "# Descriptive Statistics\n",
    "\n",
    "# Break it down by Author Order\n",
    "\n",
    "# Productiviy per Age -  broken down by Gender\n",
    "    # Metric 1: Average number of citations/publications for each age\n",
    "\n",
    "# Quantifying Dynamics of Gendered Network Formation\n",
    "# First turn everything into nodes and edges\n",
    "\n",
    "#nodes = JEL\n",
    "#https://programminghistorian.org/en/lessons/exploring-and-analyzing-network-data-with-python\n",
    "\n",
    "\n",
    "#c = conn.cursor()\n",
    "\n",
    "# New Journal IDs\n",
    "# Create table\n",
    "#c.execute('''CREATE TABLE stocks\n",
    "#             (date text, trans text, symbol text, qty real, price real)''')\n",
    "\n",
    "# Insert a row of data\n",
    "# c.execute(\"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\")\n",
    "\n",
    "# Save (commit) the changes\n",
    "#conn.commit()\n",
    "\n",
    "# We can also close the connection if we are done with it.\n",
    "# Just be sure any changes have been committed or they will be lost.\n",
    "#conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
