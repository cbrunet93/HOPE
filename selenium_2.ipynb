{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup, ResultSet\n",
    "import urllib.request\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from random import randint\n",
    "import time \n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "# http://efm.aeaweb.org/vivisimo/cgi-bin/query-meta?v%3aproject=econlit&v%3asources=All&&v%3aproject=econlit&v%3aframe=form&form=econlit-advanced&\n",
    "# https://askubuntu.com/questions/870530/how-to-install-geckodriver-in-ubuntu\n",
    "#Importing packages\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(666)\n",
    "os.chdir('/home/chris/HOPE/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHEATING\n",
    "### CHEATING\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "time.sleep(6) \n",
    "driver.get('https://libguides.liverpool.ac.uk/az.php?a=w')\n",
    "time.sleep(6) \n",
    "driver.find_element_by_xpath(u'/html/body/div[2]/div/div[4]/section[2]/div/div[1]/div[3]/div/div[2]/div[2]/div[1]/a').click()\n",
    "time.sleep(6) \n",
    "\n",
    "#driver.find_element_by_id(\"value(input1)\").send_keys(\"SO = ECONOMIC JOURNAL OR SO = ECONOMETRICA OR SO = AMERICAN ECONOMIC REVIEW OR SO = JOURNAL OF POLITICAL ECONOMY OR SO = QUARTERLY JOURNAL OF ECONOMICS OR SO = REVIEW OF ECONOMIC STUDIES\")\n",
    "#driver.find_element_by_id(\"search-button\").click()\n",
    "#driver.find_element_by_xpath(\"username\").send_keys(\"ehengel\")\n",
    "#driver.find_element_by_name(\"j_username\").send_keys(\"adoDEM88\")\n",
    "#query(IS = \"1468-0297\") # EJ\n",
    "#query(IS = \"1468-0262\") # ECA\n",
    "#query(IS = \"1944-7981\") # AER\n",
    "#query(IS = \"1537-534X\") # JPE\n",
    "#query(IS = \"1531-4650\") # QJE\n",
    "#query(IS = \"1467-937X\") # RES\n",
    "#  SO = ECONOMIC JOURNAL OR SO = ECONOMETRICA OR SO = AMERICAN ECONOMIC REVIEW OR SO = JOURNAL OF POLITICAL ECONOMY OR SO = QUARTERLY JOURNAL OF ECONOMICS OR SO = REVIEW OF ECONOMIC STUDIES\n",
    "#driver.switch_to.window(driver.window_handles[1])\n",
    "#driver.find_element_by_link_text(\"Advanced Search\").click()\n",
    "#driver.find_element_by_id(\"value(input1)\").send_keys(\"SO = ECONOMIC JOURNAL OR SO = ECONOMETRICA OR SO = AMERICAN ECONOMIC REVIEW OR SO = JOURNAL OF POLITICAL ECONOMY OR SO = QUARTERLY JOURNAL OF ECONOMICS OR SO = REVIEW OF ECONOMIC STUDIES\")\n",
    "#driver.find_element_by_id(\"search-button\").click()\n",
    "#time.sleep(6) \n",
    "#driver.switch_to.window(driver.window_handles[1])\n",
    "#driver.find_element_by_xpath(\"markFrom\").send_keys(\"11\")\n",
    "#driver.find_element_by_id(\"value(input1)\").send_keys(\"IS = \" + IS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55da06312cad4eb09c72b3ddeb0672eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1094635.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read all .txt files extracted from Web of Science\n",
    "path = '/home/chris/HOPE/data/'\n",
    "os.chdir('/home/chris/HOPE/data/')\n",
    "files = [f for f in os.listdir(path) if os.path.isfile(f)]\n",
    "data =  pd.DataFrame([])\n",
    "for file in files:\n",
    "    df = pd.read_table(file)\n",
    "    df = df.iloc[1:,:]\n",
    "    df.columns = [\"metadata\"]\n",
    "    data = data.append(df)\n",
    "    \n",
    "# Make an identifying key for each idividual article\n",
    "data[\"idx\"] = \"\"\n",
    "data.columns = [\"metadata\", \"idx\"]\n",
    "counter = Article.ArticleID.max() + 1\n",
    "for k in tqdm_notebook(range(0, len(data))):\n",
    "    string = data.iloc[k,0]\n",
    "    if string[:2] == \"PT\":\n",
    "        data.iloc[k,-1] = int(counter)\n",
    "        counter = counter +1 \n",
    "data = data.replace(r'^\\s*$', np.nan, regex=True)\n",
    "data[\"idx\"] = data[\"idx\"].ffill()\n",
    "data.to_csv(\"clean_data.csv\")\n",
    "\n",
    "# Keep all rows that ARENT already in existing .db\n",
    "data['category'] = data['metadata'].str[:2]\n",
    "WOS = data[data[\"category\"] == \"UT\"] \n",
    "WOS[\"WOS\"]= WOS['metadata'].str[3:]\n",
    "WOS_set = set(WOS.WOS)\n",
    "DOI = data[data[\"category\"] == \"DI\"] \n",
    "DOI[\"DOI\"]= DOI['metadata'].str[3:]\n",
    "DOI_set = set(DOI.DOI)\n",
    "new_DOI = list(DOI_set) + list(WOS_set)\n",
    "master_DOI = list(set(DOICorr.DOI))\n",
    "new = [x for x in new_DOI if not x in master_DOI]\n",
    "DOI = DOI[[\"idx\", \"DOI\"]]\n",
    "WOS = WOS[[\"idx\", \"WOS\"]]\n",
    "WOS.columns = [\"idx\",\"DOI\"]\n",
    "df = DOI.append(WOS)\n",
    "df = df[df['DOI'].isin(new)]\n",
    "keep = list(set(df.idx))\n",
    "df = data[data['idx'].isin(keep)]\n",
    "df.to_csv(\"clean_data_2.csv\")\n",
    "\n",
    "# Reindex \n",
    "df[\"idx\"] = \"\"\n",
    "counter = Article.ArticleID.max() + 1\n",
    "for k in tqdm_notebook(range(0, len(df))):\n",
    "    string = df.iloc[k,0]\n",
    "    if string[:2] == \"PT\":\n",
    "        df.iloc[k,-2] = int(counter)\n",
    "        counter = counter +1 \n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df[\"idx\"] = df[\"idx\"].ffill()\n",
    "df.to_csv(\"clean_data_3.csv\")\n",
    "#df = pd.read_csv(\"clean_data_3.csv\")\n",
    "#df = df.drop(columns = [\"Unnamed: 0\"])\n",
    "\n",
    "# Pick out individual authors\n",
    "df['af_flag'] = pd.np.where(df.category.str.contains(\"AF\"), \"AF\",\n",
    "                            pd.np.where(df.category.str.contains(\"TI\"), \"TI\",\n",
    "                            pd.np.where(df.category.str.contains(\"AU\"), \"AU\", \"\")))\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df[\"af_flag\"] = df[\"af_flag\"].ffill()\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df[\"idx\"] = df[\"idx\"].ffill()\n",
    "authors = df[df[\"af_flag\"] == \"AF\"]\n",
    "authors = authors['metadata'].str[3:]\n",
    "authors = pd.DataFrame(authors)\n",
    "authors = authors.drop_duplicates()\n",
    "\n",
    "# Get rid of authors that are not already in dataset\n",
    "existing_authors = set(Author.AuthorName)\n",
    "new_authors = set(authors.metadata)\n",
    "new_authors = [x for x in new_authors if not x in existing_authors]\n",
    "new_authors = pd.DataFrame(new_authors)\n",
    "new_authors[\"AuthorID\"] = range(0, len(new_authors))\n",
    "new_authors[\"AuthorID\"] = new_authors[\"AuthorID\"] + Author.AuthorID.max()\n",
    "new_authors.to_csv(\"new_authors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>AuthorID</th>\n",
       "      <th>AuthorName</th>\n",
       "      <th>Sex</th>\n",
       "      <th>NativeLanguage</th>\n",
       "      <th>PhDYear</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A. Abigail Payne</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A. Andrew John</td>\n",
       "      <td>Male</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>A. Belloni</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>A. Colin Cameron</td>\n",
       "      <td>Male</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>A. Joshua Strickland</td>\n",
       "      <td>Male</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8205</th>\n",
       "      <td>8205</td>\n",
       "      <td>9615</td>\n",
       "      <td>Edward Tower</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8206</th>\n",
       "      <td>8206</td>\n",
       "      <td>9616</td>\n",
       "      <td>Andrew Feltenstein</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8207</th>\n",
       "      <td>8207</td>\n",
       "      <td>9617</td>\n",
       "      <td>Steven Braithwait</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8208</th>\n",
       "      <td>8208</td>\n",
       "      <td>9618</td>\n",
       "      <td>John S. Pettengill</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8209</th>\n",
       "      <td>8209</td>\n",
       "      <td>9619</td>\n",
       "      <td>Michael J. Prior</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8210 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  AuthorID            AuthorName     Sex NativeLanguage  PhDYear  \\\n",
       "0         0         1      A. Abigail Payne  Female            NaN      NaN   \n",
       "1         1         2        A. Andrew John    Male        English      NaN   \n",
       "2         2         3            A. Belloni    Male            NaN      NaN   \n",
       "3         3         4      A. Colin Cameron    Male        English      NaN   \n",
       "4         4         5  A. Joshua Strickland    Male        English      NaN   \n",
       "...     ...       ...                   ...     ...            ...      ...   \n",
       "8205   8205      9615          Edward Tower    Male            NaN      NaN   \n",
       "8206   8206      9616    Andrew Feltenstein    Male            NaN      NaN   \n",
       "8207   8207      9617     Steven Braithwait    Male            NaN      NaN   \n",
       "8208   8208      9618    John S. Pettengill    Male            NaN      NaN   \n",
       "8209   8209      9619      Michael J. Prior    Male            NaN      NaN   \n",
       "\n",
       "     Note  \n",
       "0     NaN  \n",
       "1     NaN  \n",
       "2     NaN  \n",
       "3     NaN  \n",
       "4     NaN  \n",
       "...   ...  \n",
       "8205  NaN  \n",
       "8206  NaN  \n",
       "8207  NaN  \n",
       "8208  NaN  \n",
       "8209  NaN  \n",
       "\n",
       "[8210 rows x 7 columns]"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_journal(journal, start):\n",
    "   \n",
    "    time.sleep(10)\n",
    "\n",
    "    driver = webdriver.Firefox()\n",
    "    time.sleep(6) # lazy technical debt \n",
    "    driver.get('https://libguides.liverpool.ac.uk/az.php?a=w')\n",
    "    time.sleep(6) \n",
    "    driver.find_element_by_xpath(u'/html/body/main/div/section/p[3]/a').click()\n",
    "    time.sleep(6) \n",
    "    driver.find_element_by_id(\"logincaptchaform-username\").send_keys(\"erin.hengel@gmail.com\")\n",
    "    driver.find_element_by_id(\"logincaptchaform-password\").send_keys(\"wYlZYjf1e2\")\n",
    "    elem = driver.find_element_by_xpath('/html/body/main/div/section/form/div[2]/div/input')\n",
    "    actions = ActionChains(driver)\n",
    "    actions.click(elem).perform()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "\n",
    "    data = pd.DataFrame([])\n",
    "    string = journal # gross technical debt \n",
    "    string_length = len(string.split())\n",
    "    root_start = start\n",
    "     \n",
    "    for i in tqdm_notebook(range(0,(2019-root_start))): # hardcoded (technical debt)\n",
    "        \n",
    "        \n",
    "        if i > 0:\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "            start = start + 1\n",
    "\n",
    "        \n",
    "        if string_length ==  1:\n",
    "            query = \"so:\" + str(journal) + \" yearmin:\"  + str(start) \n",
    "        else: \n",
    "            query = string.split()\n",
    "            query = [\"so:\" + x  for x in query]\n",
    "            query = ' '.join(query)\n",
    "            query = query + \" yearmin:\"  + str(start) \n",
    "        print(query) \n",
    "        time.sleep(10) # Ugly workaround\n",
    "\n",
    "        if i == 0:\n",
    "            driver.find_element_by_link_text(\"Advanced\").click()\n",
    "        else:\n",
    "            driver.find_element_by_link_text(\"Modify search\").click()\n",
    "    \n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"input-query\"]').clear()\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"input-source\"]').clear()\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"input-yearmin\"]').clear()\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"input-yearmax\"]').clear()\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"input-source\"]').send_keys(journal)\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"input-yearmin\"]').send_keys(start)\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"input-yearmax\"]').send_keys(start)\n",
    "        select = Select(driver.find_element_by_xpath('/html/body/div[1]/div/div[3]/div/div/form/fieldset/table/tbody/tr[4]/td/select'))\n",
    "        select.select_by_value('1000')\n",
    "        elem = driver.find_element_by_xpath('/html/body/div[1]/div/div[3]/div/div/form/fieldset/div/input').click()\n",
    "        time.sleep(6)\n",
    "        driver.find_element_by_xpath('//*[@id=\"sel-all-top\"]').click()\n",
    "        time.sleep(6)\n",
    "        driver.find_element_by_xpath('//*[@id=\"select-all-link\"]').click()\n",
    "        time.sleep(6)\n",
    "        driver.find_element_by_xpath('//*[@id=\"export-menu-button\"]').click()\n",
    "        time.sleep(6)\n",
    "        driver.find_element_by_xpath('/html/body/div[2]/div/div[4]/div/div/div/div[2]/div[2]/div[2]/ul/li[3]/ul/li[5]/span').click()\n",
    "        time.sleep(6)\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        time.sleep(6)\n",
    "        text= driver.find_element_by_xpath(\"html\").text\n",
    "        text = pd.DataFrame([x.split(';') for x in text.split('\\n')])\n",
    "        mask = text.applymap(lambda x: x is None)\n",
    "        cols = text.columns[(mask).any()]\n",
    "        for col in text[cols]:\n",
    "            text.loc[mask[col], col] = ''\n",
    "        cols = text.columns\n",
    "        text['combined'] = text[cols].apply(lambda row: ','.join(row.values.astype(str)), axis=1)\n",
    "        text = pd.DataFrame(text.combined)\n",
    "\n",
    "        # Laziest/Ugliest workaround ever. Technical debt. \n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        for l in tqdm_notebook(range(0, len(text))):\n",
    "            string = text.iloc[l,0]\n",
    "            string = string.rstrip(\",\")\n",
    "            text.iloc[l,0] = string\n",
    "\n",
    "        data = data.append(text)\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quarterly_Journal_of_Economics = scrape_journal(\"Quarterly Journal of Economics\", 1940)\n",
    "Quarterly_Journal_of_Economics.to_csv(\"Quarterly_Journal_of_Economics.csv\")\n",
    "\n",
    "Journal_of_Political_Economy = scrape_journal(\"Journal of Political Economy\", 1940)\n",
    "Journal_of_Political_Economy.to_csv(\"Journal of Political Economy.csv\")\n",
    "\n",
    "Review_of_Economic_Studies = scrape_journal(\"Review of Economic Studies\", 1940)\n",
    "Review_of_Economic_Studies.to_csv(\"Review of Economic Studies.csv\")\n",
    "\n",
    "Econometrica = scrape_journal(\"Econometrica\", 1940)\n",
    "Econometrica.to_csv(\"Econometrica.csv\")\n",
    "\n",
    "American_Economic_Review = scrape_journal(\"American Economic Review\", 1940)\n",
    "American_Economic_Review.to_csv(\"American Economic Review.csv\")\n",
    "\n",
    "Review_of_Economics_and_Statistics = scrape_journal(\"Review of Economics and Statistics\", 1940)\n",
    "Review_of_Economics_and_Statistics.to_csv(\"Review of Economics and Statistics.csv\")\n",
    "\n",
    "Journal_of_Monetary_Economics  = scrape_journal(\"Journal of Monetary Economics\", 1975)\n",
    "Journal_of_Monetary_Economics.to_csv(\"Journal of Monetary Economics .csv\")\n",
    "\n",
    "Journal_of_Public_Economics  = scrape_journal(\"Journal of Public Economics\", 1973)\n",
    "Journal_of_Public_Economics.to_csv(\"Journal of Public Economics.csv\")\n",
    "\n",
    "Journal_of_Econometrics = scrape_journal(\"Journal of Econometrics\", 1973)\n",
    "Journal_of_Econometrics.to_csv(\"Journal of Econometrics.csv\")\n",
    "\n",
    "Journal_of_International_Economics  = scrape_journal(\"Journal of International Economics\", ??)\n",
    "Journal_of_International_Economics.to_csv(\"Journal of International Economics.csv\")\n",
    "\n",
    "Journal_of_Economic_Literature  = scrape_journal(\"Journal of Economic Literature\", 1969)\n",
    "Journal_of_Economic_Literature.to_csv(\"Journal of Economic Literature.csv\")\n",
    "\n",
    "Journal_of_Dev_Economics  = scrape_journal(\"Journal of Development Economics\", 1974)\n",
    "Journal_of_Dev_Economics.to_csv(\"Journal of Development Economics.csv\")\n",
    "\n",
    "Journal_of_Financial_Economics = scrape_journal(\"Journal of Financial Economics\", 1974)\n",
    "Journal_of_Financial_Economics.to_csv(\"Journal of Financial Economics.csv\")\n",
    "\n",
    "International_Economic_Review = scrape_journal(\"International Economic Review\", 1960)\n",
    "International_Economic_Review.to_csv(\"International Economic Review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/chris/HOPE/\") # HARDCODED \n",
    "df.to_csv(\"testtt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>AuthorID</th>\n",
       "      <th>AuthorName</th>\n",
       "      <th>Sex</th>\n",
       "      <th>NativeLanguage</th>\n",
       "      <th>PhDYear</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A. Abigail Payne</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>A. Andrew John</td>\n",
       "      <td>Male</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>A. Belloni</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>A. Colin Cameron</td>\n",
       "      <td>Male</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>A. Joshua Strickland</td>\n",
       "      <td>Male</td>\n",
       "      <td>English</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8205</th>\n",
       "      <td>8205</td>\n",
       "      <td>9615</td>\n",
       "      <td>Edward Tower</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8206</th>\n",
       "      <td>8206</td>\n",
       "      <td>9616</td>\n",
       "      <td>Andrew Feltenstein</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8207</th>\n",
       "      <td>8207</td>\n",
       "      <td>9617</td>\n",
       "      <td>Steven Braithwait</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8208</th>\n",
       "      <td>8208</td>\n",
       "      <td>9618</td>\n",
       "      <td>John S. Pettengill</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8209</th>\n",
       "      <td>8209</td>\n",
       "      <td>9619</td>\n",
       "      <td>Michael J. Prior</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8210 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  AuthorID            AuthorName     Sex NativeLanguage  PhDYear  \\\n",
       "0         0         1      A. Abigail Payne  Female            NaN      NaN   \n",
       "1         1         2        A. Andrew John    Male        English      NaN   \n",
       "2         2         3            A. Belloni    Male            NaN      NaN   \n",
       "3         3         4      A. Colin Cameron    Male        English      NaN   \n",
       "4         4         5  A. Joshua Strickland    Male        English      NaN   \n",
       "...     ...       ...                   ...     ...            ...      ...   \n",
       "8205   8205      9615          Edward Tower    Male            NaN      NaN   \n",
       "8206   8206      9616    Andrew Feltenstein    Male            NaN      NaN   \n",
       "8207   8207      9617     Steven Braithwait    Male            NaN      NaN   \n",
       "8208   8208      9618    John S. Pettengill    Male            NaN      NaN   \n",
       "8209   8209      9619      Michael J. Prior    Male            NaN      NaN   \n",
       "\n",
       "     Note  \n",
       "0     NaN  \n",
       "1     NaN  \n",
       "2     NaN  \n",
       "3     NaN  \n",
       "4     NaN  \n",
       "...   ...  \n",
       "8205  NaN  \n",
       "8206  NaN  \n",
       "8207  NaN  \n",
       "8208  NaN  \n",
       "8209  NaN  \n",
       "\n",
       "[8210 rows x 7 columns]"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/chris/HOPE/old junk/old_db/\") # HARDCODED \n",
    "\n",
    "AuthorAlias =      pd.read_csv(\"AuthorAlias.csv\")\n",
    "JEL =              pd.read_csv(\"JEL.csv\")\n",
    "Children =         pd.read_csv(\"Children.csv\")\n",
    "Nber =             pd.read_csv(\"Nber.csv\")\n",
    "NBERCorr =         pd.read_csv(\"NBERCorr.csv\")\n",
    "NBERStat =         pd.read_csv(\"NBERStat.csv\")\n",
    "sqlite_sequence =  pd.read_csv(\"sqlite_sequence.csv\")\n",
    "Inst =             pd.read_csv(\"Inst.csv\")\n",
    "sqlite_stat4 =     pd.read_csv(\"sqlite_stat4.csv\")\n",
    "InstCorr =         pd.read_csv(\"InstCorr.csv\")\n",
    "InstAlias =        pd.read_csv(\"InstAlias.csv\")\n",
    "JournalName =      pd.read_csv(\"JournalName.csv\")\n",
    "Article =          pd.read_csv(\"Article.csv\")\n",
    "ReadStat =         pd.read_csv(\"ReadStat.csv\")\n",
    "sqlite_stat1 =     pd.read_csv(\"sqlite_stat1.csv\")\n",
    "AuthorCorr =       pd.read_csv(\"AuthorCorr.csv\")\n",
    "DOICorr =          pd.read_csv(\"DOICorr.csv\")\n",
    "Author =           pd.read_csv(\"Author.csv\")\n",
    "EditorBoard =      pd.read_csv(\"EditorBoard.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7356be91f2e54d8c8ebf8eac48c03927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=999.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/chris/HOPE/')\n",
    "df = pd.read_csv(\"top5.csv\")\n",
    "df = pd.DataFrame(df.iloc[:,-1])\n",
    "df[\"title\"] = \"\"\n",
    "df.columns = [\"metadata\", \"title\"]\n",
    "df = df[~df.metadata.str.contains(\"This record is part\")]\n",
    "df = df.iloc[-1000:-1,:]\n",
    "for k in tqdm_notebook(range(0, len(df))):\n",
    "    string = df.iloc[k,0]\n",
    "    if string[:2] == \"TI\":\n",
    "        string = str(string)\n",
    "        df.iloc[k,-1] = string\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df[\"title\"] = df[\"title\"].ffill()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = df\n",
    "doi['prefix'] = df['metadata'].astype(str).str[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi= doi[doi[\"prefix\"]== \"DO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata</th>\n",
       "      <th>title</th>\n",
       "      <th>prefix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>480169</th>\n",
       "      <td>TI: Just Starting Out: Learning and Equilibriu...</td>\n",
       "      <td>TI: Just Starting Out: Learning and Equilibriu...</td>\n",
       "      <td>TI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480170</th>\n",
       "      <td>AU: Doraszelski, Ulrich, Lewis, Gregory, Pakes...</td>\n",
       "      <td>TI: Just Starting Out: Learning and Equilibriu...</td>\n",
       "      <td>AU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480171</th>\n",
       "      <td>AF: U PA, Microsoft Research New England, Harv...</td>\n",
       "      <td>TI: Just Starting Out: Learning and Equilibriu...</td>\n",
       "      <td>AF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480172</th>\n",
       "      <td>DO: http://dx.doi.org/10.1257/aer.20160177</td>\n",
       "      <td>TI: Just Starting Out: Learning and Equilibriu...</td>\n",
       "      <td>DO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480173</th>\n",
       "      <td>SO: American Economic Review, 108(3), March 20...</td>\n",
       "      <td>TI: Just Starting Out: Learning and Equilibriu...</td>\n",
       "      <td>SO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481216</th>\n",
       "      <td>KY: Asymmetric Information, Information, Litig...</td>\n",
       "      <td>TI: An Empirical Analysis of the Signaling and...</td>\n",
       "      <td>KY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481217</th>\n",
       "      <td>GD: U.S.</td>\n",
       "      <td>TI: An Empirical Analysis of the Signaling and...</td>\n",
       "      <td>GD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481218</th>\n",
       "      <td>GR: Northern America</td>\n",
       "      <td>TI: An Empirical Analysis of the Signaling and...</td>\n",
       "      <td>GR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481219</th>\n",
       "      <td>LA: English</td>\n",
       "      <td>TI: An Empirical Analysis of the Signaling and...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481220</th>\n",
       "      <td>UD: 20180712</td>\n",
       "      <td>TI: An Empirical Analysis of the Signaling and...</td>\n",
       "      <td>UD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>992 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 metadata  \\\n",
       "480169  TI: Just Starting Out: Learning and Equilibriu...   \n",
       "480170  AU: Doraszelski, Ulrich, Lewis, Gregory, Pakes...   \n",
       "480171  AF: U PA, Microsoft Research New England, Harv...   \n",
       "480172         DO: http://dx.doi.org/10.1257/aer.20160177   \n",
       "480173  SO: American Economic Review, 108(3), March 20...   \n",
       "...                                                   ...   \n",
       "481216  KY: Asymmetric Information, Information, Litig...   \n",
       "481217                                           GD: U.S.   \n",
       "481218                               GR: Northern America   \n",
       "481219                                        LA: English   \n",
       "481220                                       UD: 20180712   \n",
       "\n",
       "                                                    title prefix  \n",
       "480169  TI: Just Starting Out: Learning and Equilibriu...     TI  \n",
       "480170  TI: Just Starting Out: Learning and Equilibriu...     AU  \n",
       "480171  TI: Just Starting Out: Learning and Equilibriu...     AF  \n",
       "480172  TI: Just Starting Out: Learning and Equilibriu...     DO  \n",
       "480173  TI: Just Starting Out: Learning and Equilibriu...     SO  \n",
       "...                                                   ...    ...  \n",
       "481216  TI: An Empirical Analysis of the Signaling and...     KY  \n",
       "481217  TI: An Empirical Analysis of the Signaling and...     GD  \n",
       "481218  TI: An Empirical Analysis of the Signaling and...     GR  \n",
       "481219  TI: An Empirical Analysis of the Signaling and...     LA  \n",
       "481220  TI: An Empirical Analysis of the Signaling and...     UD  \n",
       "\n",
       "[992 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TI: Mr. Keynes and Mr. Marx</td>\n",
       "      <td>TI: Mr. Keynes and Mr. Marx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AU: Alexander, S. S.</td>\n",
       "      <td>TI: Mr. Keynes and Mr. Marx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AF: Unlisted</td>\n",
       "      <td>TI: Mr. Keynes and Mr. Marx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SO: Review of Economic Studies, 7(0), February...</td>\n",
       "      <td>TI: Mr. Keynes and Mr. Marx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DT: Journal Article</td>\n",
       "      <td>TI: Mr. Keynes and Mr. Marx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>DT: Journal Article</td>\n",
       "      <td>TI: National central banking and the internati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>PY: 1947</td>\n",
       "      <td>TI: National central banking and the internati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>DE: International Adjustment Mechanisms: Gener...</td>\n",
       "      <td>TI: National central banking and the internati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>GD: India</td>\n",
       "      <td>TI: National central banking and the internati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>GR: Asia</td>\n",
       "      <td>TI: National central banking and the internati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>991 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               metadata  \\\n",
       "10                          TI: Mr. Keynes and Mr. Marx   \n",
       "11                                 AU: Alexander, S. S.   \n",
       "12                                         AF: Unlisted   \n",
       "13    SO: Review of Economic Studies, 7(0), February...   \n",
       "14                                  DT: Journal Article   \n",
       "...                                                 ...   \n",
       "1089                                DT: Journal Article   \n",
       "1090                                           PY: 1947   \n",
       "1091  DE: International Adjustment Mechanisms: Gener...   \n",
       "1092                                          GD: India   \n",
       "1093                                           GR: Asia   \n",
       "\n",
       "                                                  title  \n",
       "10                          TI: Mr. Keynes and Mr. Marx  \n",
       "11                          TI: Mr. Keynes and Mr. Marx  \n",
       "12                          TI: Mr. Keynes and Mr. Marx  \n",
       "13                          TI: Mr. Keynes and Mr. Marx  \n",
       "14                          TI: Mr. Keynes and Mr. Marx  \n",
       "...                                                 ...  \n",
       "1089  TI: National central banking and the internati...  \n",
       "1090  TI: National central banking and the internati...  \n",
       "1091  TI: National central banking and the internati...  \n",
       "1092  TI: National central banking and the internati...  \n",
       "1093  TI: National central banking and the internati...  \n",
       "\n",
       "[991 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "991\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AF: Unlisted</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SO: Review of Economic Studies, 8(0), February...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT: Journal Article</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PY: 1941</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DE: General Equilibrium: General 2.140</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>DT: Journal Article</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>PY: 1947</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>DE: International Adjustment Mechanisms: Gener...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>GD: India</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>GR: Asia</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               metadata title\n",
       "1                                          AF: Unlisted      \n",
       "2     SO: Review of Economic Studies, 8(0), February...      \n",
       "3                                   DT: Journal Article      \n",
       "4                                              PY: 1941      \n",
       "5                DE: General Equilibrium: General 2.140      \n",
       "...                                                 ...   ...\n",
       "1089                                DT: Journal Article      \n",
       "1090                                           PY: 1947      \n",
       "1091  DE: International Adjustment Mechanisms: Gener...      \n",
       "1092                                          GD: India      \n",
       "1093                                           GR: Asia      \n",
       "\n",
       "[999 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['normal_price','final_price']]=df[['normal_price','final_price']].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticleID\n",
    "Journal\n",
    "PubDate\n",
    "Title\n",
    "Abstract\n",
    "Language\n",
    "Volume\n",
    "Issue\n",
    "FirstPage\n",
    "LastPage\n",
    "DOI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onlyine supplements \n",
    "# Online Supplement\n",
    "# Report of the Editor\n",
    "# Editors' Introduction\n",
    "# Foreward\n",
    "\n",
    "# Connect to SQL Database and reads every Table in the whole DB\n",
    "# Then converts each table into a .csv file so we can work with themn in Python\n",
    "\n",
    "# Top 5 Workaround (tech debt) because the original webcrawler did chunks of 2 years instead of 1, need to refactor\n",
    "# Sooo.. this cell finds and deletes all those  wrongly webscraped duplicates\n",
    "# Take 4 hours for loop to run... it's certainly not optimized \n",
    "\n",
    "#path = '/home/chris/HOPE/data/top5'\n",
    "#all_files = glob.glob(path + \"/*.csv\")\n",
    "#li = []\n",
    "#for filename in all_files:\n",
    "#    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "#    li.append(df)\n",
    "#df = pd.concat(li, axis=0, ignore_index=True)\n",
    "#df[\"duplicates\"] = \"\"\n",
    "#for k in tqdm_notebook(range(0, len(df))):\n",
    "#    string = df.iloc[k,1]\n",
    "###    if string[:2] == \"TI\":\n",
    "#        string = str(string)\n",
    "#        df.iloc[k,-1] = string\n",
    "#df[\"duplicates\"] = df[\"duplicates\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "#df[\"duplicates\"] = df[\"duplicates\"].ffill()\n",
    "#df.duplicates = df.combined + df.duplicates\n",
    "#df = df.drop_duplicates(subset='duplicates', keep=\"first\")\n",
    "#df = df.combined\n",
    "#df.to_csv(\"top5.csv\")\n",
    "\n",
    "# Create master dataframe\n",
    "\n",
    "#path = '/home/chris/HOPE/'\n",
    "#all_files = glob.glob(path + \"/*.csv\")\n",
    "#li = []\n",
    "#df = pd.DataFrame(df.iloc[:,-1])for filename in all_files:\n",
    "#    df = pd.read_csv#(filename, index_col=None, header=0)\n",
    "#    li.append(df)\n",
    "#df = pd.concat(li, axis=0, ignore_index=True)\n",
    "#\n",
    "#def to_csv():\n",
    "#\n",
    "#    os.chdir(\"/home/chris/HOPE/\") # HARDCODED \n",
    "#    db = sqlite3.connect('read.db')\n",
    "#    os.chdir(\"/home/chris/HOPE/old_db\") # HARDCODED \n",
    "#    cursor = db.cursor()\n",
    "#    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "#    tables = cursor.fetchall()\n",
    "#    for table_name in tqdm_notebook(tables):\n",
    "#        table_name = table_name[0]\n",
    "#        table = pd.read_sql_query(\"SELECT * from %s\" % table_name, db)\n",
    "#        table.to_csv(table_name + '.csv', index_label='index')\n",
    "#    cursor.close()\n",
    "#    db.close()\n",
    "#\n",
    "#to_csv()\n",
    "\n",
    "# Journal 1: AER\n",
    "\n",
    "#def scrape_aer(base_url, latest_volume, latest_issue):\n",
    " #   \n",
    "#    # Step 1: Find  urls for all issues/volumes\n",
    "#    html_page = urllib.request.urlopen(base_url)\n",
    "#    soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "#    links = pd.DataFrame([])\n",
    "#    for link in soup.find_all('a', href=True): # Grab all the URL hyperlinks on the webpage\n",
    "#        links = links.append(pd.DataFrame(pd.Series(link['href']))) \n",
    "#    links.columns = ['url']\n",
    "#    links = links[links['url'].str.contains(\"/issues/\")]\n",
    "    \n",
    "#    # Step 2: Find urls for all individual articles\n",
    "#    papers = pd.DataFrame([])\n",
    "#    for i in tqdm_notebook(range(0, len(links))): \n",
    "#        # for i in tqdm_notebook(range(0, 1)): \n",
    "#        # sleep(randint(3,10)) # Technical debt: optimize sleep timer\n",
    "#3        new_url = \"https://www.aeaweb.org\" + str(links.iloc[i,0])\n",
    "#        html_page = urllib.request.urlopen(new_url)\n",
    "#        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "#        for paper in tqdm_notebook(soup.find_all('a', href=True)): # Grab all the URL hyperlinks on the webpage\n",
    "##            papers = papers.append(pd.DataFrame(pd.Series(paper['href'])))\n",
    "#    papers.columns = [\"url\"]\n",
    "#   papers = papers[papers['url'].str.contains(\"/articles\")]\n",
    "    \n",
    "\n",
    "#    return(papers)\n",
    "\n",
    "\n",
    "#df = scrape_aer(base_url = \"https://www.aeaweb.org/journals/aer/issues\", latest_volume = latest_aer_issue, latest_issue = latest_aer_issue)\n",
    "\n",
    "\n",
    "   # return(links)\n",
    "#for i in range(0, 4):\n",
    "#    url = df.iloc[i,0]\n",
    "#    url =  \"https://www.aeaweb.org\" + str(url )\n",
    "#    if url[-1] == \"i\":\n",
    "#        continue\n",
    "#    print(url)\n",
    "#    \n",
    "#    # Turn into soup \n",
    "#    html_page = urllib.request.urlopen(url)\n",
    "#    soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "#    \n",
    "#    # Title\n",
    "#    title  = str(soup.findAll(\"h1\", {\"class\": \"title\"}))\n",
    "#    title = title[19:-6] # hardcoded\n",
    "#    \n",
    "#    # Authors\n",
    "#    authors = str(soup.findAll(\"li\", {\"class\": \"author\"}))\n",
    "#    authors = authors[50:-6] # hardcoded]\n",
    "#    authors = pd.DataFrame(authors.split(\",\"))\n",
    "#    for j in range(0, len(authors)):\n",
    "#        string = str(authors.iloc[j,0])\n",
    "#        if j == 0:\n",
    "#            string = string[3:-5] #harcoded\n",
    "#        if j > 0: \n",
    "#            string = string.replace('</li>','')\n",
    "#            string = string[50:]\n",
    "#            string = string.strip()\n",
    "#        authors.iloc[j,0] = string\n",
    "#    \n",
    "#    \n",
    "#    # Authors#\n",
    "\n",
    "    #title = title[19:-6]\n",
    "    # df = title\n",
    "    #print(doi)\n",
    "    #title = pd.DataFrame(pd.Series(str(author[1:-1]))) # HARDCODED \n",
    "    \n",
    "    # PROBLEM RE: AFFILIATION # Junk / Under-construction\n",
    "\n",
    "#def scrape_repec(journal_name, root_url, pages, latest_volume, latest_issue):\n",
    "\n",
    "#    \"\"\"\n",
    "#    This is a webcrawler that crawls through every page/article of a\n",
    "#    TO DO: Make the webcrawler  \"smarter\" so that it can detect that there are 17 pages of data to scrape\n",
    "#    TO DO: Scrape more than 5 journals\n",
    "#    \"\"\"#\n",
    "#\n",
    "#    # Soupa nd link \n",
    "#    links = pd.DataFrame([])\n",
    "#    for i in tqdm_notebook(range(1, pages)):\n",
    "#        url = root_url       \n",
    "#        if i > 1:  # \"1\" workaround is due to their uniue naming convenitons\n",
    "#            url = url[:-5] + str(i) + url[-5:]\n",
    "#        html_page = urllib.request.urlopen(url)\n",
    "#        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "#        for link in soup.find_all('a', href=True): # Grab all the URL hyperlinks on the webpage\n",
    "#            links = links.append(pd.DataFrame(pd.Series(link['href']))) \n",
    "#    links.columns = [\"url\"]\n",
    "#    links = links[links['url'].str.contains(\"/a/\")]\n",
    "#    links = \"https://ideas.repec.org\" + links\n",
    "#    links[\"metadata\"] = links[\"url\"]\n",
    "#\n",
    "#    # Metadata manipulation; haervests volume, issue, year, and pages of the scrape d ata\n",
    "#    # TECHNICAL DEBT! These 6 lines are ugly and gross\n",
    "#    links[\"metadata\"] = links[\"metadata\"].str.split(\"v\", n=1, expand=True)[1]\n",
    "#    links[\"Volume\"] = links[\"metadata\"].str.split(\"y\", n=1, expand=True)[0]\n",
    "#    links[\"metadata\"] = links[\"metadata\"].str.split(\"y\", n=1, expand=True)[1]\n",
    "#    links[\"y\"] = links[\"metadata\"].str.split(\"i\", n=1, expand=True)[0]\n",
    "#    links[\"metadata\"] = links[\"metadata\"].str.split(\"i\", n=1, expand=True)[1]\n",
    "#    links[\"Issue\"] = links[\"metadata\"].str.split(\"p\", n=1, expand=True)[0]\n",
    "##    links[\"metadata\"] = links[\"metadata\"].str.split(\"p\", n=1, expand=True)[1]\n",
    "#    links[\"FirstPage\"] = links[\"metadata\"].str.split(\"-\", n=1, expand=True)[0]\n",
    "#    links[\"metadata\"] = links[\"metadata\"].str.split(\"-\", n=1, expand=True)[1]\n",
    "##    links[\"LastPage\"] = links[\"metadata\"].str.split(\".\", n=1, expand=True)[0]\n",
    " #   links = links.drop(columns = [\"metadata\"])\n",
    " #   links[\"Volume\"] = pd.to_numeric(links[\"Volume\"])\n",
    " #   links = links[links[\"Volume\"] > latest_volume] # CUTOFF FOR NEWEST ONLY \n",
    " #   \n",
    " #   # Now we loop through all the individual URLs to scrape some more specific metadata\n",
    " #   # Question: Do citations from the original data soruce hold up?\n",
    " #   data = pd.DataFrame([]) # intialize empty dfs to be filled up in loop\n",
    " #   authors = pd.DataFrame([])\n",
    " #   for url in tqdm_notebook(links[\"url\"]):\n",
    " #       df = links[links[\"url\"] == url]\n",
    " #       html_page = urllib.request.urlopen(url)\n",
    " #       soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "        \n",
    "        # Author(s)        \n",
    "  #      author = str(soup.find(id =\"author\"))\n",
    "  #      author = pd.DataFrame(pd.Series(str(author[1:-1]))) # HARDCODED \n",
    "        # Question - how to get institution?\n",
    "        # Question - Institution \n",
    "        \n",
    "  #      # Title\n",
    "  #      title = str(soup.find(id =\"title\"))\n",
    "  #      title = str(title[21:-11]) # HARDCODED \n",
    "   #     df[\"Title\"] = title\n",
    "\n",
    "        # Abstract        \n",
    "    #    abstract = str(soup.find(id =\"abstract-body\"))\n",
    "    #    abstract = str(abstract[24:-6]) # HARDCODED \n",
    "    #    df[\"Abstract\"] = abstract\n",
    "\n",
    "        # Citations\n",
    "     #   citations = str(soup.find(id =\"cites-tab\"))\n",
    "     #   if len(citations) == 4:\n",
    "     #       df[\"CiteCount\"] = 0\n",
    "     #   else:\n",
    "     #       citations = str(citations.split(\"role\")[1])\n",
    "     #       citations = str(citations.split(\"Citations\")[0])\n",
    "     ##       citations = str(citations.split(\">\")[1])\n",
    "      #3#      df[\"CiteCount\"] = citations\n",
    "       # \n",
    "        # Create empty columns (quick/dirty workaround) that formats table such that you can append it to SQL \n",
    "       # df[\"Journal\"] = journal_name\n",
    "       # df[\"index\"] = \"MISSING\"\n",
    "       # df[\"ArticleID\"] = \"MISSING\"\n",
    "       # df[\"Language\"] = \"MISSING\"\n",
    "       # df[\"Received\"] = \"MISSING\"\n",
    "       # df[\"Accepted\"] = \"MISSING\"\n",
    "       # df[\"Part\"] = \"MISSING\"\n",
    "       # df[\"Comments\"] = \"MISSING\"\n",
    "       # df[\"Note\"] = \"MISSING\"\n",
    "       # df[\"PubDate\"] = \"MISSING\"\n",
    "       # df = df.drop(columns = [\"y\", \"url\"]) #  Technical debt \n",
    "       #3 \n",
    "       #3 # Append!\n",
    "       ## data = data.append(df) \n",
    "        #authors = authors.append(author)\n",
    " #   data = data[Article.columns] # HARDCODED \n",
    " #   data = pd.DataFrame(data)\n",
    "    \n",
    "  #  return(data, authors)\n",
    "\n",
    "# TOMROROW: FInish all 17 tables\n",
    "#qje, authors = scrape_repec(journal_name = \"QJE\", root_url = \"https://ideas.repec.org/s/oup/qjecon.html\", pages = 10, latest_volume = latest_qje_volume, latest_issue = latest_qje_issue) \n",
    "# Write to folder (WORKAROUND!)\n",
    "\n",
    "#os.chdir(\"/home/chris/HOPE/new_db\") # HARDCODED \n",
    "#qje.to_csv(\"Article.csv\")\n",
    "#aer = scrape_article_urls(journal_name = \"AER\",root_url = \"https://ideas.repec.org/s/aea/aecrev.html\", pages = 10, latest_volume = latest_aer_volume, latest_issue = latest_aer_issue)\n",
    "#jpe = scrape_article_urls(journal_name = \"JPE\",root_url = \"https://ideas.repec.org/s/ucp/jpolec.html\", pages = 10, latest_volume = latest_jpe_volume, latest_issue = latest_jpe_issue)\n",
    "#eca = scrape_article_urls(journal_name = \"ECA\",root_url = \"https://ideas.repec.org/s/ecm/emetrp.html\", pages = 10, latest_volume = latest_eca_volume, latest_issue = latest_eca_issue)\n",
    "#res = scrape_article_urls(journal_name = \"RES\",root_url = \"https://ideas.repec.org/s/oup/restud.html.html\", pages = 10, latest_jres_volume = 2, latest_issue = latest_res_issue)\n",
    "\n",
    "#html_page = urllib.request.urlopen(\"https://ideas.repec.org/a/oup/qjecon/v125y2010i2p729-765..html\")\n",
    "#soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "#        # Author(s)        \n",
    "#author = str(soup.find(id =\"listed-authors\"))\n",
    "#print(author)\n",
    "#author = pd.DataFrame(pd.Series(str(author[1:-1]))) # HARDCODED \n",
    "#authorr = pd.DataFrame(pd.Series(str(author.split(\"authorname\",1))))\n",
    "#authorr.iloc[0,0] = authorr.iloc[0,0][82:-20] #HARDCODED (gross)\n",
    "#\n",
    "#print(authorr.iloc[0,0])\n",
    "#\n",
    "#    print(authorr[i])\n",
    "# JEL Codes per Journal - how have they evolved over time?\n",
    "# Technichal debt - can be made much sleeker\n",
    "#set(JournalName.Journal)\n",
    "#ECA_articles = set(Article[Article[\"Journal\"] == \"ECA\"].iloc[:,1])\n",
    "#AER_articles = set(Article[Article[\"Journal\"] == \"AER\"].iloc[:,1])\n",
    "#JPE_articles = set(Article[Article[\"Journal\"] == \"JPE\"].iloc[:,1])\n",
    "#RES_articles = set(Article[Article[\"Journal\"] == \"RES\"].iloc[:,1])\n",
    "#QJE_articles = set(Article[Article[\"Journal\"] == \"QJE\"].iloc[:,1])\n",
    "#top5_articles = [ECA_articles, AER_articles, JPE_articles, RES_articles, QJE_articles]\n",
    "\n",
    "# Make flags for every JEL code via loop\n",
    "# I figure we might need these flags for fixed effects or something at some point\n",
    "#df = Article # I just like working with df better\n",
    "#articles = set(Article.ArticleID)\n",
    "#JEL_codes =\n",
    "#for article in tqdm(articles):\n",
    "\n",
    "# Now split this graph into two\n",
    "#    chunk = JEL[JEL[\"ArticleID\"] == article]\n",
    "#    moving average of JEL codes per year per journal\n",
    "#\n",
    "#    print(df)\n",
    "#    print(article)\n",
    "\n",
    "# That's a picture for the entier thign, yes but how have they evolved over time?\n",
    "# 1 Pie Chart Per Decade Per Journal of JEL Codes\n",
    "# Investigate articles with zero JEL codes. Is that something I have to go get?\n",
    "\n",
    "#JEL\n",
    "#JEL_tally = JEL.groupby(\"JEL\").count()\n",
    "#JEL_tally = JEL_tally.iloc[:,1]\n",
    "#print( JEL_tally )\n",
    "\n",
    "# The growth of the averga page length over time\n",
    "\n",
    "# Difference in page length between men and women\n",
    "\n",
    "# Exploratory regressions\n",
    "\n",
    "# Descriptive Statistics\n",
    "\n",
    "# Break it down by Author Order\n",
    "\n",
    "# Productiviy per Age -  broken down by Gender\n",
    "    # Metric 1: Average number of citations/publications for each age\n",
    "\n",
    "# Quantifying Dynamics of Gendered Network Formation\n",
    "# First turn everything into nodes and edges\n",
    "\n",
    "#nodes = JEL\n",
    "#https://programminghistorian.org/en/lessons/exploring-and-analyzing-network-data-with-python\n",
    "\n",
    "\n",
    "#c = conn.cursor()\n",
    "\n",
    "# New Journal IDs\n",
    "# Create table\n",
    "#c.execute('''CREATE TABLE stocks\n",
    "#             (date text, trans text, symbol text, qty real, price real)''')\n",
    "\n",
    "# Insert a row of data\n",
    "# c.execute(\"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\")\n",
    "\n",
    "# Save (commit) the changes\n",
    "#conn.commit()\n",
    "\n",
    "# We can also close the connection if we are done with it.\n",
    "# Just be sure any changes have been committed or they will be lost.\n",
    "#conn.close()\n",
    "    #print(\"Landing Page...\")\n",
    "    #driver.get('https://www.aeaweb.org/econlit/')\n",
    "    #driver.find_element_by_link_text(\"Log In\").click()\n",
    "    #time.sleep(5) \n",
    "    #print(\"Dashboard..\")\n",
    "    #driver.find_element_by_id(\"logincaptchaform-username\").send_keys(\"erin.hengel@gmail.com\")\n",
    "    #3driver.find_element_by_id(\"logincaptchaform-password\").send_keys(\"wYlZYjf1e2\")\n",
    "    #elem = driver.find_element_by_xpath('/html/body/main/div/section/form/div[2]/div/input')\n",
    "    #actions = ActionChains(driver)\n",
    "    #actions.click(elem).perform()\n",
    "    #time.sleep(5) \n",
    "    #print(\"AEA for Members..\")\n",
    "    #driver.get('http://efm.aeaweb.org/vivisimo/cgi-bin/query-meta?v:project=econlit&v:frame=form&frontpage=1')\n",
    "    #driver.quit()\n",
    "    \n",
    "    \n",
    "    \n",
    "#def find_latest(journal):\n",
    " #   \n",
    "#    \"\"\"\n",
    "#    This function finds the \"newest\" metadata and saves it as variables that we use later on\n",
    "#    We then use these variables later on to automate thresholds and cutoffs\n",
    "#    This is done bcause we specficially do NOT WANT to grab any data prior to our cutoff\n",
    "#    \"\"\"\n",
    "#    \n",
    "#    data = Article[Article[\"Journal\"] == journal]\n",
    "#    data = data.sort_values(by=[\"Volume\", \"Issue\"])\n",
    " #   data = pd.DataFrame(data.iloc[-1,:])\n",
    "#    data = data.T\n",
    "#    volume = int(data[\"Volume\"])\n",
    "#    issue = int(data[\"Issue\"])\n",
    "\n",
    " #   return(volume, issue)\n",
    "\n",
    "#latest_eca_volume, latest_eca_issue = find_latest(\"ECA\")\n",
    "#latest_aer_volume, latest_aer_issue = find_latest(\"AER\")\n",
    "#latest_jpe_volume, latest_jpe_issue = find_latest(\"JPE\")\n",
    "#latest_qje_volume, latest_qje_issue = find_latest(\"QJE\")\n",
    "#latest_res_volume, latest_res_issue = find_latest(\"RES\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
