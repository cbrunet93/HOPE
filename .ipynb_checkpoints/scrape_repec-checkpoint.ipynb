{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup, ResultSet\n",
    "import urllib.request\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from random import randint\n",
    "import time \n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "# http://efm.aeaweb.org/vivisimo/cgi-bin/query-meta?v%3aproject=econlit&v%3asources=All&&v%3aproject=econlit&v%3aframe=form&form=econlit-advanced&\n",
    "# https://askubuntu.com/questions/870530/how-to-install-geckodriver-in-ubuntu\n",
    "#Importing packages\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(666)\n",
    "os.chdir('/home/chris/HOPE/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_journal(journal, start):\n",
    "   \n",
    "    time.sleep(10)\n",
    "\n",
    "    driver = webdriver.Firefox()\n",
    "    time.sleep(6) # lazy technical debt \n",
    "    driver.get('https://www.aeaweb.org/econlit/econlit-for-members')\n",
    "    time.sleep(6) \n",
    "    driver.find_element_by_xpath(u'/html/body/main/div/section/p[3]/a').click()\n",
    "    time.sleep(6) \n",
    "    driver.find_element_by_id(\"logincaptchaform-username\").send_keys(\"erin.hengel@gmail.com\")\n",
    "    driver.find_element_by_id(\"logincaptchaform-password\").send_keys(\"wYlZYjf1e2\")\n",
    "    elem = driver.find_element_by_xpath('/html/body/main/div/section/form/div[2]/div/input')\n",
    "    actions = ActionChains(driver)\n",
    "    actions.click(elem).perform()\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "\n",
    "    data = pd.DataFrame([])\n",
    "    string = journal # gross technical debt \n",
    "    string_length = len(string.split())\n",
    "    root_start = start\n",
    "     \n",
    "    for i in tqdm_notebook(range(0,(2019-root_start))): # hardcoded (technical debt)\n",
    "        \n",
    "        \n",
    "        if i > 0:\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "            start = start + 1\n",
    "\n",
    "        \n",
    "        if string_length ==  1:\n",
    "            query = \"so:\" + str(journal) + \" yearmin:\"  + str(start) \n",
    "        else: \n",
    "            query = string.split()\n",
    "            query = [\"so:\" + x  for x in query]\n",
    "            query = ' '.join(query)\n",
    "            query = query + \" yearmin:\"  + str(start) \n",
    "        print(query) \n",
    "        time.sleep(10) # Ugly workaround\n",
    "\n",
    "        if i == 0:\n",
    "            driver.find_element_by_link_text(\"Advanced\").click()\n",
    "        else:\n",
    "            driver.find_element_by_link_text(\"Modify search\").click()\n",
    "    \n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"input-query\"]').clear()\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"input-source\"]').clear()\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"input-yearmin\"]').clear()\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"input-yearmax\"]').clear()\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"input-source\"]').send_keys(journal)\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"input-yearmin\"]').send_keys(start)\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"input-yearmax\"]').send_keys(start)\n",
    "        select = Select(driver.find_element_by_xpath('/html/body/div[1]/div/div[3]/div/div/form/fieldset/table/tbody/tr[4]/td/select'))\n",
    "        select.select_by_value('1000')\n",
    "        elem = driver.find_element_by_xpath('/html/body/div[1]/div/div[3]/div/div/form/fieldset/div/input').click()\n",
    "        time.sleep(6)\n",
    "        driver.find_element_by_xpath('//*[@id=\"sel-all-top\"]').click()\n",
    "        time.sleep(6)\n",
    "        driver.find_element_by_xpath('//*[@id=\"select-all-link\"]').click()\n",
    "        time.sleep(6)\n",
    "        driver.find_element_by_xpath('//*[@id=\"export-menu-button\"]').click()\n",
    "        time.sleep(6)\n",
    "        driver.find_element_by_xpath('/html/body/div[2]/div/div[4]/div/div/div/div[2]/div[2]/div[2]/ul/li[3]/ul/li[5]/span').click()\n",
    "        time.sleep(6)\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        time.sleep(6)\n",
    "        text= driver.find_element_by_xpath(\"html\").text\n",
    "        text = pd.DataFrame([x.split(';') for x in text.split('\\n')])\n",
    "        mask = text.applymap(lambda x: x is None)\n",
    "        cols = text.columns[(mask).any()]\n",
    "        for col in text[cols]:\n",
    "            text.loc[mask[col], col] = ''\n",
    "        cols = text.columns\n",
    "        text['combined'] = text[cols].apply(lambda row: ','.join(row.values.astype(str)), axis=1)\n",
    "        text = pd.DataFrame(text.combined)\n",
    "\n",
    "        # Laziest/Ugliest workaround ever. Technical debt. \n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        text['combined'] = text['combined'].str.replace(',,',',')\n",
    "        for l in tqdm_notebook(range(0, len(text))):\n",
    "            string = text.iloc[l,0]\n",
    "            string = string.rstrip(\",\")\n",
    "            text.iloc[l,0] = string\n",
    "\n",
    "        data = data.append(text)\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Quarterly_Journal_of_Economics = scrape_journal(\"Quarterly Journal of Economics\", 1940)\n",
    "Quarterly_Journal_of_Economics.to_csv(\"Quarterly_Journal_of_Economics.csv\")\n",
    "\n",
    "Journal_of_Political_Economy = scrape_journal(\"Journal of Political Economy\", 1940)\n",
    "Journal_of_Political_Economy.to_csv(\"Journal of Political Economy.csv\")\n",
    "\n",
    "Review_of_Economic_Studies = scrape_journal(\"Review of Economic Studies\", 1940)\n",
    "Review_of_Economic_Studies.to_csv(\"Review of Economic Studies.csv\")\n",
    "\n",
    "Econometrica = scrape_journal(\"Econometrica\", 1940)\n",
    "Econometrica.to_csv(\"Econometrica.csv\")\n",
    "\n",
    "American_Economic_Review = scrape_journal(\"American Economic Review\", 1940)\n",
    "American_Economic_Review.to_csv(\"American Economic Review.csv\")\n",
    "\n",
    "Review_of_Economics_and_Statistics = scrape_journal(\"Review of Economics and Statistics\", 1940)\n",
    "Review_of_Economics_and_Statistics.to_csv(\"Review of Economics and Statistics.csv\")\n",
    "\n",
    "Journal_of_Monetary_Economics  = scrape_journal(\"Journal of Monetary Economics\", 1975)\n",
    "Journal_of_Monetary_Economics.to_csv(\"Journal of Monetary Economics .csv\")\n",
    "\n",
    "Journal_of_Public_Economics  = scrape_journal(\"Journal of Public Economics\", 1973)\n",
    "Journal_of_Public_Economics.to_csv(\"Journal of Public Economics.csv\")\n",
    "\n",
    "Journal_of_Econometrics = scrape_journal(\"Journal of Econometrics\", 1973)\n",
    "Journal_of_Econometrics.to_csv(\"Journal of Econometrics.csv\")\n",
    "\n",
    "Journal_of_International_Economics  = scrape_journal(\"Journal of International Economics\", ??)\n",
    "Journal_of_International_Economics.to_csv(\"Journal of International Economics.csv\")\n",
    "\n",
    "Journal_of_Economic_Literature  = scrape_journal(\"Journal of Economic Literature\", 1969)\n",
    "Journal_of_Economic_Literature.to_csv(\"Journal of Economic Literature.csv\")\n",
    "\n",
    "Journal_of_Dev_Economics  = scrape_journal(\"Journal of Development Economics\", 1974)\n",
    "Journal_of_Dev_Economics.to_csv(\"Journal of Development Economics.csv\")\n",
    "\n",
    "Journal_of_Financial_Economics = scrape_journal(\"Journal of Financial Economics\", 1974)\n",
    "Journal_of_Financial_Economics.to_csv(\"Journal of Financial Economics.csv\")\n",
    "\n",
    "International_Economic_Review = scrape_journal(\"International Economic Review\", 1960)\n",
    "International_Economic_Review.to_csv(\"International Economic Review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Top 5 Workaround (tech debt) because the original webcrawler did chunks of 2 years instead of 1, need to refactor\n",
    "# Sooo.. this cell finds and deletes all those  wrongly webscraped duplicates\n",
    "# Take 4 hours for loop to run... it's certainly not optimized \n",
    "\n",
    "path = '/home/chris/HOPE/data/top5'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "li = []\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n",
    "df[\"duplicates\"] = \"\"\n",
    "for k in tqdm_notebook(range(0, len(df))):\n",
    "    string = df.iloc[k,1]\n",
    "    if string[:2] == \"TI\":\n",
    "        string = str(string)\n",
    "        df.iloc[k,-1] = string\n",
    "df[\"duplicates\"] = df[\"duplicates\"].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df[\"duplicates\"] = df[\"duplicates\"].ffill()\n",
    "df.duplicates = df.combined + df.duplicates\n",
    "df = df.drop_duplicates(subset='duplicates', keep=\"first\")\n",
    "df = df.combined\n",
    "df.to_csv(\"top5.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create master dataframe\n",
    "\n",
    "path = '/home/chris/HOPE/data/'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "li = []\n",
    "df = pd.DataFrame(df.iloc[:,-1])for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "df = pd.concat(li, axis=0, ignore_index=True)\n",
    "df = pd.DataFrame(df.iloc[:,-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d740d072f4f4bc1b3f415b892daf5ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1136843.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"title\"] = \"\"\n",
    "for k in tqdm_notebook(range(0, len(df))):\n",
    "    string = df.iloc[k,0]\n",
    "    if string[:2] == \"TI\":\n",
    "        string = str(string)\n",
    "        df.iloc[k,-1] = string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArticleID\n",
    "Journal\n",
    "PubDate\n",
    "Title\n",
    "Abstract\n",
    "Language\n",
    "Volume\n",
    "Issue\n",
    "FirstPage\n",
    "LastPage\n",
    "DOI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onlyine supplements \n",
    "# Online Supplement\n",
    "# Report of the Editor\n",
    "# Editors' Introduction\n",
    "# Foreward\n",
    "\n",
    "# Connect to SQL Database and reads every Table in the whole DB\n",
    "# Then converts each table into a .csv file so we can work with themn in Python\n",
    "\n",
    "#def to_csv():\n",
    "#\n",
    "#    os.chdir(\"/home/chris/HOPE/\") # HARDCODED \n",
    "#    db = sqlite3.connect('read.db')\n",
    "#    os.chdir(\"/home/chris/HOPE/old_db\") # HARDCODED \n",
    "#    cursor = db.cursor()\n",
    "#    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "#    tables = cursor.fetchall()\n",
    "#    for table_name in tqdm_notebook(tables):\n",
    "#        table_name = table_name[0]\n",
    "#        table = pd.read_sql_query(\"SELECT * from %s\" % table_name, db)\n",
    "#        table.to_csv(table_name + '.csv', index_label='index')\n",
    "#    cursor.close()\n",
    "#    db.close()\n",
    "#\n",
    "#to_csv()\n",
    "#os.chdir(\"/home/chris/HOPE/old_db\") # HARDCODED \n",
    "# Load the data you just found (technical debt?)\n",
    "\n",
    "#AuthorAlias =      pd.read_csv(\"AuthorAlias.csv\")\n",
    "#JEL =              pd.read_csv(\"JEL.csv\")\n",
    "#Children =         pd.read_csv(\"Children.csv\")\n",
    "#Nber =             pd.read_csv(\"Nber.csv\")\n",
    "#NBERCorr =         pd.read_csv(\"NBERCorr.csv\")\n",
    "#NBERStat =         pd.read_csv(\"NBERStat.csv\")\n",
    "##sqlite_sequence =  pd.read_csv(\"sqlite_sequence.csv\")\n",
    "#Inst =             pd.read_csv(\"Inst.csv\")\n",
    "#sqlite_stat4 =     pd.read_csv(\"sqlite_stat4.csv\")\n",
    "#InstCorr =         pd.read_csv(\"InstCorr.csv\")\n",
    "#InstAlias =        pd.read_csv(\"InstAlias.csv\")\n",
    "##JournalName =      pd.read_csv(\"JournalName.csv\")\n",
    "#Article =          pd.read_csv(\"Article.csv\")\n",
    "#ReadStat =         pd.read_csv(\"ReadStat.csv\")\n",
    "#sqlite_stat1 =     pd.read_csv(\"sqlite_stat1.csv\")\n",
    "#AuthorCorr =       pd.read_csv(\"AuthorCorr.csv\")\n",
    "#DOICorr =          pd.read_csv(\"DOICorr.csv\")\n",
    "#Author =           pd.read_csv(\"Author.csv\")\n",
    "#EditorBoard =      pd.read_csv(\"EditorBoard.csv\")\n",
    "\n",
    "# Journal 1: AER\n",
    "\n",
    "#def scrape_aer(base_url, latest_volume, latest_issue):\n",
    " #   \n",
    "#    # Step 1: Find  urls for all issues/volumes\n",
    "#    html_page = urllib.request.urlopen(base_url)\n",
    "#    soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "#    links = pd.DataFrame([])\n",
    "#    for link in soup.find_all('a', href=True): # Grab all the URL hyperlinks on the webpage\n",
    "#        links = links.append(pd.DataFrame(pd.Series(link['href']))) \n",
    "#    links.columns = ['url']\n",
    "#    links = links[links['url'].str.contains(\"/issues/\")]\n",
    "    \n",
    "#    # Step 2: Find urls for all individual articles\n",
    "#    papers = pd.DataFrame([])\n",
    "#    for i in tqdm_notebook(range(0, len(links))): \n",
    "#        # for i in tqdm_notebook(range(0, 1)): \n",
    "#        # sleep(randint(3,10)) # Technical debt: optimize sleep timer\n",
    "#3        new_url = \"https://www.aeaweb.org\" + str(links.iloc[i,0])\n",
    "#        html_page = urllib.request.urlopen(new_url)\n",
    "#        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "#        for paper in tqdm_notebook(soup.find_all('a', href=True)): # Grab all the URL hyperlinks on the webpage\n",
    "##            papers = papers.append(pd.DataFrame(pd.Series(paper['href'])))\n",
    "#    papers.columns = [\"url\"]\n",
    "#   papers = papers[papers['url'].str.contains(\"/articles\")]\n",
    "    \n",
    "\n",
    "#    return(papers)\n",
    "\n",
    "\n",
    "#df = scrape_aer(base_url = \"https://www.aeaweb.org/journals/aer/issues\", latest_volume = latest_aer_issue, latest_issue = latest_aer_issue)\n",
    "\n",
    "\n",
    "   # return(links)\n",
    "#for i in range(0, 4):\n",
    "#    url = df.iloc[i,0]\n",
    "#    url =  \"https://www.aeaweb.org\" + str(url )\n",
    "#    if url[-1] == \"i\":\n",
    "#        continue\n",
    "#    print(url)\n",
    "#    \n",
    "#    # Turn into soup \n",
    "#    html_page = urllib.request.urlopen(url)\n",
    "#    soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "#    \n",
    "#    # Title\n",
    "#    title  = str(soup.findAll(\"h1\", {\"class\": \"title\"}))\n",
    "#    title = title[19:-6] # hardcoded\n",
    "#    \n",
    "#    # Authors\n",
    "#    authors = str(soup.findAll(\"li\", {\"class\": \"author\"}))\n",
    "#    authors = authors[50:-6] # hardcoded]\n",
    "#    authors = pd.DataFrame(authors.split(\",\"))\n",
    "#    for j in range(0, len(authors)):\n",
    "#        string = str(authors.iloc[j,0])\n",
    "#        if j == 0:\n",
    "#            string = string[3:-5] #harcoded\n",
    "#        if j > 0: \n",
    "#            string = string.replace('</li>','')\n",
    "#            string = string[50:]\n",
    "#            string = string.strip()\n",
    "#        authors.iloc[j,0] = string\n",
    "#    \n",
    "#    \n",
    "#    # Authors#\n",
    "\n",
    "    #title = title[19:-6]\n",
    "    # df = title\n",
    "    #print(doi)\n",
    "    #title = pd.DataFrame(pd.Series(str(author[1:-1]))) # HARDCODED \n",
    "    \n",
    "    # PROBLEM RE: AFFILIATION # Junk / Under-construction\n",
    "\n",
    "#def scrape_repec(journal_name, root_url, pages, latest_volume, latest_issue):\n",
    "\n",
    "#    \"\"\"\n",
    "#    This is a webcrawler that crawls through every page/article of a\n",
    "#    TO DO: Make the webcrawler  \"smarter\" so that it can detect that there are 17 pages of data to scrape\n",
    "#    TO DO: Scrape more than 5 journals\n",
    "#    \"\"\"#\n",
    "#\n",
    "#    # Soupa nd link \n",
    "#    links = pd.DataFrame([])\n",
    "#    for i in tqdm_notebook(range(1, pages)):\n",
    "#        url = root_url       \n",
    "#        if i > 1:  # \"1\" workaround is due to their uniue naming convenitons\n",
    "#            url = url[:-5] + str(i) + url[-5:]\n",
    "#        html_page = urllib.request.urlopen(url)\n",
    "#        soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "#        for link in soup.find_all('a', href=True): # Grab all the URL hyperlinks on the webpage\n",
    "#            links = links.append(pd.DataFrame(pd.Series(link['href']))) \n",
    "#    links.columns = [\"url\"]\n",
    "#    links = links[links['url'].str.contains(\"/a/\")]\n",
    "#    links = \"https://ideas.repec.org\" + links\n",
    "#    links[\"metadata\"] = links[\"url\"]\n",
    "#\n",
    "#    # Metadata manipulation; haervests volume, issue, year, and pages of the scrape d ata\n",
    "#    # TECHNICAL DEBT! These 6 lines are ugly and gross\n",
    "#    links[\"metadata\"] = links[\"metadata\"].str.split(\"v\", n=1, expand=True)[1]\n",
    "#    links[\"Volume\"] = links[\"metadata\"].str.split(\"y\", n=1, expand=True)[0]\n",
    "#    links[\"metadata\"] = links[\"metadata\"].str.split(\"y\", n=1, expand=True)[1]\n",
    "#    links[\"y\"] = links[\"metadata\"].str.split(\"i\", n=1, expand=True)[0]\n",
    "#    links[\"metadata\"] = links[\"metadata\"].str.split(\"i\", n=1, expand=True)[1]\n",
    "#    links[\"Issue\"] = links[\"metadata\"].str.split(\"p\", n=1, expand=True)[0]\n",
    "##    links[\"metadata\"] = links[\"metadata\"].str.split(\"p\", n=1, expand=True)[1]\n",
    "#    links[\"FirstPage\"] = links[\"metadata\"].str.split(\"-\", n=1, expand=True)[0]\n",
    "#    links[\"metadata\"] = links[\"metadata\"].str.split(\"-\", n=1, expand=True)[1]\n",
    "##    links[\"LastPage\"] = links[\"metadata\"].str.split(\".\", n=1, expand=True)[0]\n",
    " #   links = links.drop(columns = [\"metadata\"])\n",
    " #   links[\"Volume\"] = pd.to_numeric(links[\"Volume\"])\n",
    " #   links = links[links[\"Volume\"] > latest_volume] # CUTOFF FOR NEWEST ONLY \n",
    " #   \n",
    " #   # Now we loop through all the individual URLs to scrape some more specific metadata\n",
    " #   # Question: Do citations from the original data soruce hold up?\n",
    " #   data = pd.DataFrame([]) # intialize empty dfs to be filled up in loop\n",
    " #   authors = pd.DataFrame([])\n",
    " #   for url in tqdm_notebook(links[\"url\"]):\n",
    " #       df = links[links[\"url\"] == url]\n",
    " #       html_page = urllib.request.urlopen(url)\n",
    " #       soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "        \n",
    "        # Author(s)        \n",
    "  #      author = str(soup.find(id =\"author\"))\n",
    "  #      author = pd.DataFrame(pd.Series(str(author[1:-1]))) # HARDCODED \n",
    "        # Question - how to get institution?\n",
    "        # Question - Institution \n",
    "        \n",
    "  #      # Title\n",
    "  #      title = str(soup.find(id =\"title\"))\n",
    "  #      title = str(title[21:-11]) # HARDCODED \n",
    "   #     df[\"Title\"] = title\n",
    "\n",
    "        # Abstract        \n",
    "    #    abstract = str(soup.find(id =\"abstract-body\"))\n",
    "    #    abstract = str(abstract[24:-6]) # HARDCODED \n",
    "    #    df[\"Abstract\"] = abstract\n",
    "\n",
    "        # Citations\n",
    "     #   citations = str(soup.find(id =\"cites-tab\"))\n",
    "     #   if len(citations) == 4:\n",
    "     #       df[\"CiteCount\"] = 0\n",
    "     #   else:\n",
    "     #       citations = str(citations.split(\"role\")[1])\n",
    "     #       citations = str(citations.split(\"Citations\")[0])\n",
    "     ##       citations = str(citations.split(\">\")[1])\n",
    "      #3#      df[\"CiteCount\"] = citations\n",
    "       # \n",
    "        # Create empty columns (quick/dirty workaround) that formats table such that you can append it to SQL \n",
    "       # df[\"Journal\"] = journal_name\n",
    "       # df[\"index\"] = \"MISSING\"\n",
    "       # df[\"ArticleID\"] = \"MISSING\"\n",
    "       # df[\"Language\"] = \"MISSING\"\n",
    "       # df[\"Received\"] = \"MISSING\"\n",
    "       # df[\"Accepted\"] = \"MISSING\"\n",
    "       # df[\"Part\"] = \"MISSING\"\n",
    "       # df[\"Comments\"] = \"MISSING\"\n",
    "       # df[\"Note\"] = \"MISSING\"\n",
    "       # df[\"PubDate\"] = \"MISSING\"\n",
    "       # df = df.drop(columns = [\"y\", \"url\"]) #  Technical debt \n",
    "       #3 \n",
    "       #3 # Append!\n",
    "       ## data = data.append(df) \n",
    "        #authors = authors.append(author)\n",
    " #   data = data[Article.columns] # HARDCODED \n",
    " #   data = pd.DataFrame(data)\n",
    "    \n",
    "  #  return(data, authors)\n",
    "\n",
    "# TOMROROW: FInish all 17 tables\n",
    "#qje, authors = scrape_repec(journal_name = \"QJE\", root_url = \"https://ideas.repec.org/s/oup/qjecon.html\", pages = 10, latest_volume = latest_qje_volume, latest_issue = latest_qje_issue) \n",
    "# Write to folder (WORKAROUND!)\n",
    "\n",
    "#os.chdir(\"/home/chris/HOPE/new_db\") # HARDCODED \n",
    "#qje.to_csv(\"Article.csv\")\n",
    "#aer = scrape_article_urls(journal_name = \"AER\",root_url = \"https://ideas.repec.org/s/aea/aecrev.html\", pages = 10, latest_volume = latest_aer_volume, latest_issue = latest_aer_issue)\n",
    "#jpe = scrape_article_urls(journal_name = \"JPE\",root_url = \"https://ideas.repec.org/s/ucp/jpolec.html\", pages = 10, latest_volume = latest_jpe_volume, latest_issue = latest_jpe_issue)\n",
    "#eca = scrape_article_urls(journal_name = \"ECA\",root_url = \"https://ideas.repec.org/s/ecm/emetrp.html\", pages = 10, latest_volume = latest_eca_volume, latest_issue = latest_eca_issue)\n",
    "#res = scrape_article_urls(journal_name = \"RES\",root_url = \"https://ideas.repec.org/s/oup/restud.html.html\", pages = 10, latest_jres_volume = 2, latest_issue = latest_res_issue)\n",
    "\n",
    "#html_page = urllib.request.urlopen(\"https://ideas.repec.org/a/oup/qjecon/v125y2010i2p729-765..html\")\n",
    "#soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "#        # Author(s)        \n",
    "#author = str(soup.find(id =\"listed-authors\"))\n",
    "#print(author)\n",
    "#author = pd.DataFrame(pd.Series(str(author[1:-1]))) # HARDCODED \n",
    "#authorr = pd.DataFrame(pd.Series(str(author.split(\"authorname\",1))))\n",
    "#authorr.iloc[0,0] = authorr.iloc[0,0][82:-20] #HARDCODED (gross)\n",
    "#\n",
    "#print(authorr.iloc[0,0])\n",
    "#\n",
    "#    print(authorr[i])\n",
    "# JEL Codes per Journal - how have they evolved over time?\n",
    "# Technichal debt - can be made much sleeker\n",
    "#set(JournalName.Journal)\n",
    "#ECA_articles = set(Article[Article[\"Journal\"] == \"ECA\"].iloc[:,1])\n",
    "#AER_articles = set(Article[Article[\"Journal\"] == \"AER\"].iloc[:,1])\n",
    "#JPE_articles = set(Article[Article[\"Journal\"] == \"JPE\"].iloc[:,1])\n",
    "#RES_articles = set(Article[Article[\"Journal\"] == \"RES\"].iloc[:,1])\n",
    "#QJE_articles = set(Article[Article[\"Journal\"] == \"QJE\"].iloc[:,1])\n",
    "#top5_articles = [ECA_articles, AER_articles, JPE_articles, RES_articles, QJE_articles]\n",
    "\n",
    "# Make flags for every JEL code via loop\n",
    "# I figure we might need these flags for fixed effects or something at some point\n",
    "#df = Article # I just like working with df better\n",
    "#articles = set(Article.ArticleID)\n",
    "#JEL_codes =\n",
    "#for article in tqdm(articles):\n",
    "\n",
    "# Now split this graph into two\n",
    "#    chunk = JEL[JEL[\"ArticleID\"] == article]\n",
    "#    moving average of JEL codes per year per journal\n",
    "#\n",
    "#    print(df)\n",
    "#    print(article)\n",
    "\n",
    "# That's a picture for the entier thign, yes but how have they evolved over time?\n",
    "# 1 Pie Chart Per Decade Per Journal of JEL Codes\n",
    "# Investigate articles with zero JEL codes. Is that something I have to go get?\n",
    "\n",
    "#JEL\n",
    "#JEL_tally = JEL.groupby(\"JEL\").count()\n",
    "#JEL_tally = JEL_tally.iloc[:,1]\n",
    "#print( JEL_tally )\n",
    "\n",
    "# The growth of the averga page length over time\n",
    "\n",
    "# Difference in page length between men and women\n",
    "\n",
    "# Exploratory regressions\n",
    "\n",
    "# Descriptive Statistics\n",
    "\n",
    "# Break it down by Author Order\n",
    "\n",
    "# Productiviy per Age -  broken down by Gender\n",
    "    # Metric 1: Average number of citations/publications for each age\n",
    "\n",
    "# Quantifying Dynamics of Gendered Network Formation\n",
    "# First turn everything into nodes and edges\n",
    "\n",
    "#nodes = JEL\n",
    "#https://programminghistorian.org/en/lessons/exploring-and-analyzing-network-data-with-python\n",
    "\n",
    "\n",
    "#c = conn.cursor()\n",
    "\n",
    "# New Journal IDs\n",
    "# Create table\n",
    "#c.execute('''CREATE TABLE stocks\n",
    "#             (date text, trans text, symbol text, qty real, price real)''')\n",
    "\n",
    "# Insert a row of data\n",
    "# c.execute(\"INSERT INTO stocks VALUES ('2006-01-05','BUY','RHAT',100,35.14)\")\n",
    "\n",
    "# Save (commit) the changes\n",
    "#conn.commit()\n",
    "\n",
    "# We can also close the connection if we are done with it.\n",
    "# Just be sure any changes have been committed or they will be lost.\n",
    "#conn.close()\n",
    "    #print(\"Landing Page...\")\n",
    "    #driver.get('https://www.aeaweb.org/econlit/')\n",
    "    #driver.find_element_by_link_text(\"Log In\").click()\n",
    "    #time.sleep(5) \n",
    "    #print(\"Dashboard..\")\n",
    "    #driver.find_element_by_id(\"logincaptchaform-username\").send_keys(\"erin.hengel@gmail.com\")\n",
    "    #3driver.find_element_by_id(\"logincaptchaform-password\").send_keys(\"wYlZYjf1e2\")\n",
    "    #elem = driver.find_element_by_xpath('/html/body/main/div/section/form/div[2]/div/input')\n",
    "    #actions = ActionChains(driver)\n",
    "    #actions.click(elem).perform()\n",
    "    #time.sleep(5) \n",
    "    #print(\"AEA for Members..\")\n",
    "    #driver.get('http://efm.aeaweb.org/vivisimo/cgi-bin/query-meta?v:project=econlit&v:frame=form&frontpage=1')\n",
    "    #driver.quit()\n",
    "    \n",
    "    \n",
    "    \n",
    "#def find_latest(journal):\n",
    " #   \n",
    "#    \"\"\"\n",
    "#    This function finds the \"newest\" metadata and saves it as variables that we use later on\n",
    "#    We then use these variables later on to automate thresholds and cutoffs\n",
    "#    This is done bcause we specficially do NOT WANT to grab any data prior to our cutoff\n",
    "#    \"\"\"\n",
    "#    \n",
    "#    data = Article[Article[\"Journal\"] == journal]\n",
    "#    data = data.sort_values(by=[\"Volume\", \"Issue\"])\n",
    " #   data = pd.DataFrame(data.iloc[-1,:])\n",
    "#    data = data.T\n",
    "#    volume = int(data[\"Volume\"])\n",
    "#    issue = int(data[\"Issue\"])\n",
    "\n",
    " #   return(volume, issue)\n",
    "\n",
    "#latest_eca_volume, latest_eca_issue = find_latest(\"ECA\")\n",
    "#latest_aer_volume, latest_aer_issue = find_latest(\"AER\")\n",
    "#latest_jpe_volume, latest_jpe_issue = find_latest(\"JPE\")\n",
    "#latest_qje_volume, latest_qje_issue = find_latest(\"QJE\")\n",
    "#latest_res_volume, latest_res_issue = find_latest(\"RES\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
