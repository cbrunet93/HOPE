{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup, ResultSet\n",
    "import urllib.request\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from random import randint\n",
    "import time \n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import Select\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(666)\n",
    "path = '/home/chris/HOPE/data/' # Technical debt ; hardcoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# http://efm.aeaweb.org/vivisimo/cgi-bin/query-meta?v%3aproject=econlit&v%3asources=All&&v%3aproject=econlit&v%3aframe=form&form=econlit-advanced&\n",
    "# https://askubuntu.com/questions/870530/how-to-install-geckodriver-in-ubuntu\n",
    "\n",
    "# This chunk that connects to the backend is commented out, because it is code you only have to run once, initially \n",
    "# files = [f for f in os.listdir(path) if os.path.isfile(f)]\n",
    "# data =  pd.DataFrame([])\n",
    "# for file in tqdm_notebook(files):\n",
    "#    df = pd.read_table(file)\n",
    "#    df = df.iloc[1:,:]\n",
    "#    if len(df.columns) > 1:\n",
    "#        continue\n",
    "#    df.columns = [\"metadata\"]\n",
    "#    data = data.append(df)\n",
    "    #path = '/home/chris/HOPE/'\n",
    "# all_files = glob.glob(path + \"/*.csv\")\n",
    "#li = []\n",
    "#df = pd.DataFrame(df.iloc[:,-1])for filename in all_files:\n",
    "#    df = pd.read_csv#(filename, index_col=None, header=0)\n",
    "#    li.append(df)\n",
    "#df = pd.concat(li, axis=0, ignore_index=True)\n",
    "#\n",
    "\n",
    "#def to_csv():\n",
    "\n",
    "#    cursor = db.cursor()\n",
    "#    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "#    tables = cursor.fetchall()\n",
    "#    for table_name in tqdm_notebook(tables):\n",
    "#        table_name = table_name[0]\n",
    "#        table = pd.read_sql_query(\"SELECT * from %s\" % table_name, db)\n",
    "#        table.to_csv(table_name + '.csv', index_label='index')\n",
    "#    cursor.close()\n",
    "#    db.close()\n",
    "#\n",
    "#to_csv()\n",
    "\n",
    "\n",
    "# After running it once, it makes more practical sense to save every \"working file\" as a csv\n",
    "# And then turn these csv files into dataframes\n",
    "os.chdir(\"/home/chris/HOPE/data/old junk/old_db/\") # HARDCODED \n",
    "Article =          pd.read_csv(\"Article.csv\")\n",
    "AuthorAlias =      pd.read_csv(\"AuthorAlias.csv\")\n",
    "JEL =              pd.read_csv(\"JEL.csv\")\n",
    "Children =         pd.read_csv(\"Children.csv\")\n",
    "Nber =             pd.read_csv(\"Nber.csv\")\n",
    "NBERCorr =         pd.read_csv(\"NBERCorr.csv\")\n",
    "NBERStat =         pd.read_csv(\"NBERStat.csv\")\n",
    "sqlite_sequence =  pd.read_csv(\"sqlite_sequence.csv\")\n",
    "Inst =             pd.read_csv(\"Inst.csv\")\n",
    "sqlite_stat4 =     pd.read_csv(\"sqlite_stat4.csv\")\n",
    "InstCorr =         pd.read_csv(\"InstCorr.csv\")\n",
    "InstAlias =        pd.read_csv(\"InstAlias.csv\")\n",
    "JournalName =      pd.read_csv(\"JournalName.csv\")\n",
    "ReadStat =         pd.read_csv(\"ReadStat.csv\")\n",
    "sqlite_stat1 =     pd.read_csv(\"sqlite_stat1.csv\")\n",
    "AuthorCorr =       pd.read_csv(\"AuthorCorr.csv\")\n",
    "DOICorr =          pd.read_csv(\"DOICorr.csv\")\n",
    "Author =           pd.read_csv(\"Author.csv\")\n",
    "EditorBoard =      pd.read_csv(\"EditorBoard.csv\")\n",
    "os.chdir('/home/chris/HOPE/data/')  # HARDCODED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files = [f for f in os.listdir(path) if os.path.isfile(f)]\n",
    "data =  pd.DataFrame([])\n",
    "for file in tqdm_notebook(files):\n",
    "    df = pd.read_table(file)\n",
    "    df = df.iloc[1:,:]\n",
    "    if len(df.columns) > 1:\n",
    "        continue\n",
    "    df.columns = [\"metadata\"]\n",
    "    data = data.append(df)\n",
    "# Make an identifying key for each idividual article\n",
    "data[\"idx\"] = \"\"\n",
    "data.columns = [\"metadata\", \"idx\"]\n",
    "counter = Article.ArticleID.max() + 1\n",
    "for k in tqdm_notebook(range(0, len(data))):\n",
    "    string = data.iloc[k,0]\n",
    "    if string[:2] == \"PT\":\n",
    "        data.iloc[k,-1] = int(counter)\n",
    "        counter = counter +1 \n",
    "data = data.replace(r'^\\s*$', np.nan, regex=True)\n",
    "data[\"idx\"] = data[\"idx\"].ffill()\n",
    "\n",
    "# Keep all rows that ARENT already in existing .db\n",
    "data['category'] = data['metadata'].str[:2]\n",
    "WOS = data[data[\"category\"] == \"UT\"] \n",
    "WOS[\"WOS\"]= WOS['metadata'].str[3:]\n",
    "WOS_set = set(WOS.WOS)\n",
    "DOI = data[data[\"category\"] == \"DI\"] \n",
    "DOI[\"DOI\"]= DOI['metadata'].str[3:]\n",
    "DOI_set = set(DOI.DOI)\n",
    "new_DOI = list(DOI_set) + list(WOS_set)\n",
    "master_DOI = list(set(DOICorr.DOI))\n",
    "new = [x for x in new_DOI if not x in master_DOI]\n",
    "DOI = DOI[[\"idx\", \"DOI\"]]\n",
    "WOS = WOS[[\"idx\", \"WOS\"]]\n",
    "WOS.columns = [\"idx\",\"DOI\"]\n",
    "df = DOI.append(WOS)\n",
    "df = df[df['DOI'].isin(new)]\n",
    "keep = list(set(df.idx))\n",
    "df = data[data['idx'].isin(keep)]\n",
    "\n",
    "# Reindex \n",
    "# WARNING: THIS TAKES 20 HOURS FOR SOME REASON\n",
    "df[\"idx\"] = \"\"\n",
    "counter = Article.ArticleID.max() + 1\n",
    "for k in tqdm_notebook(range(0, len(df))):\n",
    "    string = df.iloc[k,0]\n",
    "    if string[:2] == \"PT\":\n",
    "        df.iloc[k,-2] = int(counter)\n",
    "        counter = counter +1 \n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df[\"idx\"] = df[\"idx\"].ffill()\n",
    "df.to_csv(\"clean_data_3.csv\")\n",
    "\n",
    "# http://efm.aeaweb.org/vivisimo/cgi-bin/query-meta?v%3aproject=econlit&v%3asources=All&&v%3aproject=econlit&v%3aframe=form&form=econlit-advanced&\n",
    "# https://askubuntu.com/questions/870530/how-to-install-geckodriver-in-ubuntu\n",
    "### Cheating/Workaround\n",
    "#driver = webdriver.Firefox()\n",
    "#time.sleep(6) \n",
    "#driver.get('https://libguides.liverpool.ac.uk/az.php?a=w')\n",
    "#time.sleep(6) \n",
    "#driver.find_element_by_xpath(u'/html/body/div[2]/div/div[4]/section[2]/div/div[1]/div[3]/div/div[2]/div[2]/div[1]/a').click()\n",
    "#time.sleep(6) \n",
    "#driver.find_element_by_id(\"value(input1)\").send_keys(\"SO = ECONOMIC JOURNAL OR SO = ECONOMETRICA OR SO = AMERICAN ECONOMIC REVIEW OR SO = JOURNAL OF POLITICAL ECONOMY OR SO = QUARTERLY JOURNAL OF ECONOMICS OR SO = REVIEW OF ECONOMIC STUDIES\")\n",
    "#driver.find_element_by_id(\"search-button\").click()\n",
    "#driver.find_element_by_xpath(\"username\").send_keys(\"ehengel\")\n",
    "#driver.find_element_by_name(\"j_username\").send_keys(\"adoDEM88\")\n",
    "#query(IS = \"1468-0297\") # EJ\n",
    "#query(IS = \"1468-0262\") # ECA\n",
    "#query(IS = \"1944-7981\") # AER\n",
    "#query(IS = \"1537-534X\") # JPE\n",
    "#query(IS = \"1531-4650\") # QJE\n",
    "#query(IS = \"1467-937X\") # RES\n",
    "#  SO = ECONOMIC JOURNAL OR SO = ECONOMETRICA OR SO = AMERICAN ECONOMIC REVIEW OR SO = JOURNAL OF POLITICAL ECONOMY OR SO = QUARTERLY JOURNAL OF ECONOMICS OR SO = REVIEW OF ECONOMIC STUDIES\n",
    "#driver.switch_to.window(driver.window_handles[1])\n",
    "#driver.find_element_by_link_text(\"Advanced Search\").click()\n",
    "#driver.find_element_by_id(\"value(input1)\").send_keys(\"SO = ECONOMIC JOURNAL OR SO = ECONOMETRICA OR SO = AMERICAN ECONOMIC REVIEW OR SO = JOURNAL OF POLITICAL ECONOMY OR SO = QUARTERLY JOURNAL OF ECONOMICS OR SO = REVIEW OF ECONOMIC STUDIES\")\n",
    "#driver.find_element_by_id(\"search-button\").click()\n",
    "#time.sleep(6) \n",
    "#driver.switch_to.window(driver.window_handles[1])\n",
    "#driver.find_element_by_xpath(\"markFrom\").send_keys(\"11\")\n",
    "#driver.find_element_by_id(\"value(input1)\").send_keys(\"IS = \" + IS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff42dfb977b04285a7e2bdd1d0decd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=17189.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0d6446af3342b2bb065c7e3e1e3f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26553.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301916d53c5d4a6cb75ef16ec0fae7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26553.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Data from checkpoint\n",
    "os.chdir(\"/home/chris/HOPE/data\") # HARDCODED \n",
    "df = pd.read_csv(\"clean_data_3.csv\")\n",
    "df = df.drop(columns = [\"Unnamed: 0\"]) \n",
    "\n",
    "# Pick out individual authors\n",
    "df['af_flag'] = pd.np.where(df.category == \"AF\", \"AF\",pd.np.where(df.category == \"TI\", \"TI\", pd.np.where(df.category == \"AU\", \"AU\", \"\")))\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df[\"af_flag\"] = df[\"af_flag\"].ffill()\n",
    "df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "df[\"idx\"] = df[\"idx\"].ffill()\n",
    "#df = df.iloc[1:1000,:]\n",
    "\n",
    "# Initialize loop parameters\n",
    "articles = pd.DataFrame(set(df.idx))\n",
    "article_id_counter = Article.ArticleID.max() +1\n",
    "article_index_counter = Article.index.max() +1\n",
    "\n",
    "# Initialize empty df's to be filled up in loop\n",
    "new_authors = pd.DataFrame([])\n",
    "new_articles = pd.DataFrame([])\n",
    "\n",
    "for i in tqdm_notebook(range(0, len(articles))):\n",
    "    \n",
    "    # Initialize data\n",
    "    article = pd.DataFrame([])\n",
    "    data = df[df[\"idx\"] == articles.iloc[i,0]]\n",
    "    data['category'] = data['category'].astype(str)\n",
    "    \n",
    "    # Initialize counting/looping mechanism \n",
    "    article[\"index\"] = pd.Series(article_index_counter)\n",
    "    article[\"ArticleID\"] = pd.Series(article_id_counter)\n",
    "    \n",
    "    # Journal name \n",
    "    journal = data[data[\"category\"] == \"SO\"]\n",
    "    journal = str(journal.iloc[0,0])\n",
    "    journal = journal[3:]\n",
    "    \n",
    "    # Dealing with special issues \n",
    "    #special_issue = data[data[\"category\"] == \"SI\"]\n",
    "    #if len(special_issue) > 0:\n",
    "    #    #print(\"Caution - Special Issue - \" + str(special_issue))\n",
    "    #    special_issue = str(special_issue.iloc[0,0])\n",
    "    \n",
    "    # Manual workaround for naming conventions \n",
    "    if journal == \"AMERICAN ECONOMIC REVIEW\":\n",
    "        journal = \"AER\"\n",
    "    if journal == \"ECONOMIC JOURNAL\":\n",
    "        journal = \"EJ\"\n",
    "    if journal == \"JOURNAL OF POLITICAL ECONOMY\":\n",
    "        journal = \"JPE\"\n",
    "    if journal == \"ECONOMETRICA\":\n",
    "        journal = \"ECA\"\n",
    "    if journal == \"QUARTERLY JOURNAL OF ECONOMICS\":\n",
    "        journal = \"QJE\"\n",
    "    if journal == \"REVIEW OF ECONOMIC STUDIES\":\n",
    "        journal = \"RES\"\n",
    "    article[\"Journal\"] = journal\n",
    "    \n",
    "    # Publication Date\n",
    "    pubdate = data[data[\"category\"] == \"PY\"]\n",
    "    if len(pubdate) > 0:\n",
    "        pubdate = str(pubdate.iloc[0,0])\n",
    "        pubdate = pubdate[3:]\n",
    "    else:\n",
    "        pubdate = \"MISSING\"\n",
    "    article[\"PubDate\"] = pubdate\n",
    "    \n",
    "    # Publication Dates\n",
    "    title = data[data[\"category\"] == \"TI\"]\n",
    "    title = str(title.iloc[0,0])\n",
    "    title = title[3:]\n",
    "    article[\"Title\"] = title\n",
    "\n",
    "    # Authors\n",
    "    authors = data[data[\"af_flag\"] == \"AF\"]\n",
    "    authors = pd.DataFrame(authors['metadata'].str[3:])\n",
    "    authors[\"ArticleID\"] = article_id_counter\n",
    "    authors[\"index\"] = pd.Series(article_index_counter)\n",
    "    \n",
    "    # Abstracts\n",
    "    article[\"Abstract\"] = \"\"\n",
    "    Abstract = data[data[\"category\"] == \"AB\"] # naming conventions technical debt \n",
    "    if len(Abstract) < 1:\n",
    "        article[\"Abstract\"] = \"MISSING\"\n",
    "    else: \n",
    "        Abstract = str(Abstract.iloc[0,0])\n",
    "        Abstract = Abstract[3:]\n",
    "        article[\"Abstract\"] = Abstract\n",
    "    \n",
    "    # Harcode The next several columns\n",
    "    article[\"Language\"] = \"English\"\n",
    "    article[\"Received\"] = \"MISSING\"\n",
    "    article[\"Accepted\"] = \"MISSING\"\n",
    "\n",
    "    # Volume \n",
    "    article[\"Volume\"] = \"\"\n",
    "    Volume = data[data[\"category\"] == \"VL\"] # naming conventions technical debt \n",
    "    if len(Volume) < 1:\n",
    "        article[\"Volume\"] = \"MISSING\"\n",
    "    else: \n",
    "        Volume = str(Volume.iloc[0,0])\n",
    "        Volume = Volume[3:]\n",
    "        article[\"Volume\"] = Volume\n",
    "\n",
    "    # Issue\n",
    "    article[\"Issue\"] = \"\"\n",
    "    Issue = data[data[\"category\"] == \"IS\"] # naming conventions technical debt \n",
    "   # print(Issue)\n",
    "    if len(Issue) < 1:\n",
    "        article[\"Issue\"] = \"MISSING\"\n",
    "    else: \n",
    "        Issue = str(Issue.iloc[0,0])\n",
    "        Issue = Issue[3:]\n",
    "        article[\"Issue\"] = Issue\n",
    "        \n",
    "    # Final few columns\n",
    "    article[\"Part\"] = \"MISSING\"\n",
    "    \n",
    "    # First Page \n",
    "    article[\"FirstPage\"] = \"\"\n",
    "    FirstPage = data[data[\"category\"] == \"BP\"] # naming conventions technical debt \n",
    "    if len(FirstPage) < 1:\n",
    "        article[\"FirstPage\"] = \"MISSING\"\n",
    "    else: \n",
    "        FirstPage = str(FirstPage.iloc[0,0])\n",
    "        FirstPage = FirstPage[3:]\n",
    "        article[\"FirstPage\"] = FirstPage\n",
    "        \n",
    "    # Last Page \n",
    "    article[\"LastPage \"] = \"\"\n",
    "    LastPage = data[data[\"category\"] == \"EP\"] # naming conventions technical debt \n",
    "    if len(LastPage) < 1:\n",
    "        article[\"LastPage\"] = \"MISSING\"\n",
    "    else: \n",
    "        LastPage = str(LastPage.iloc[0,0])\n",
    "        LastPage = LastPage[3:]\n",
    "        article[\"LastPage\"] = LastPage\n",
    "    \n",
    "    # Comments\n",
    "    article[\"Comments\"] = \"MISSING\"\n",
    "    \n",
    "    # Citations\n",
    "    article[\"CiteCount \"] = \"\"\n",
    "    CiteCount = data[data[\"category\"] == \"NR\"] # naming conventions technical debt \n",
    "    if len(LastPage) < 1:\n",
    "        article[\"CiteCount\"] = \"MISSING\"\n",
    "    else: \n",
    "        CiteCount = str(CiteCount.iloc[0,0])\n",
    "        CiteCount = CiteCount[3:]\n",
    "        article[\"CiteCount\"] = CiteCount        \n",
    "    \n",
    "    # Note\n",
    "    article[\"Note\"] = \"MISSING\"\n",
    "\n",
    "    # update counter mechanism \n",
    "    article_id_counter += 1\n",
    "    article_index_counter += 1 # redundant technical debt \n",
    "\n",
    "    # Append in loop\n",
    "    new_articles = new_articles.append(article)\n",
    "    new_authors = new_authors.append(authors)\n",
    "    \n",
    "    \n",
    "# Us data manipulation is wrangling all the output \n",
    "# (In the next cell, we will proceed to append these new dataframes onto read.db!)\n",
    "for i in tqdm_notebook(range(0, len(new_authors))):\n",
    "    string = new_authors.iloc[i,0]\n",
    "    string =  string.title()\n",
    "    new_authors.iloc[i,0] = string\n",
    "new_authors['metadata'] = new_authors['metadata'].astype('str') #where the max length is set at 80 bytes,\n",
    "new_authors[\"last name\"] = \" \"\n",
    "new_authors[\"first name\"] = \" \"\n",
    "\n",
    "#\n",
    "for i in tqdm_notebook(range(0, len(new_authors))):\n",
    "    string = new_authors.iloc[i,0]\n",
    "    last = string.split(\",\")[0]\n",
    "    try:\n",
    "        first = string.split(\",\")[1]\n",
    "    except IndexError:\n",
    "        first = \"MISSING\"\n",
    "    new_authors.iloc[i,-1] = first\n",
    "    new_authors.iloc[i,-2] = last\n",
    "new_authors = new_authors.reset_index(drop=True)\n",
    "cols = [-3, -6]\n",
    "new_articles = new_articles.drop(new_articles.columns[[-3,-6]], axis = 1)\n",
    "new_authors[\"AuthorName\"] = new_authors[\"first name\"] + \" \" + new_authors[\"last name\"] \n",
    "\n",
    "# Workaround \n",
    "master_key = pd.read_csv(\"key.csv\", header = None)\n",
    "master_key.columns = [\"metadata\", \"clean name\", \"confirmed name\", \"Sex\"]\n",
    "author_corr = new_authors.merge(master_key, on = \"metadata\", how = \"outer\")\n",
    "author_corr.to_csv(\"AuthorCorr_temp.csv\")\n",
    "\n",
    "#Now format tables identical to those found in \"read.db\"\n",
    "df=  author_corr\n",
    "df = df[[\"confirmed name\", \"Sex\"]] \n",
    "df.columns = [\"AuthorName\", \"Sex\"]\n",
    "df[\"NativeLanguage\"] = \"\"\n",
    "df[\"PhDYear\"] = \"\"\n",
    "df[\"Note\"] = \"\"\n",
    "df = df.drop_duplicates()\n",
    "redundant = pd.DataFrame(set(df.AuthorName).intersection(set(Author.AuthorName)))\n",
    "redundant.columns=[\"AuthorName\"]\n",
    "df = df[~df['AuthorName'].isin(set(redundant.AuthorName))]\n",
    "df = df.dropna()\n",
    "df.to_csv(\"new_authors_temporary_3.csv\") # Naming conventions :) \n",
    "new_articles.to_csv(\"new_articles_temporary_3.csv\") # Naming conventi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20418afac6f4ca6881f7878aeff9939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This Cell Connects to the .db file and updates it \n",
    "\n",
    "# uickload the data \n",
    "df = pd.read_csv(\"new_authors_temporary_3.csv\")\n",
    "df = df.drop(columns = [\"Unnamed: 0\"])\n",
    "df.insert(0,'AuthorID','')\n",
    "df['AuthorID'] = range(0 + max(Author.AuthorID) + 1, len(df) + max(Author.AuthorID)+1)\n",
    "new_articles = pd.read_csv(\"new_articles_temporary_3.csv\")\n",
    "new_articles = new_articles.drop(columns = [\"Unnamed: 0\"])\n",
    "\n",
    "# Update Author Table by Dynamically Inserting into a Database\n",
    "for row in tqdm_notebook(new_articles.iterrows()):\n",
    "    \n",
    "    # Technical debt: surely there is a smarter way to do this \n",
    "    row = pd.DataFrame(row)\n",
    "    row = pd.DataFrame(row.iloc[1,0])\n",
    "    ArticleID = row.iloc[0,0]\n",
    "    Journal = row.iloc[1,0]\n",
    "    PubDate = row.iloc[2,0]\n",
    "    Title = row.iloc[3,0]\n",
    "    Abstract = row.iloc[4,0]\n",
    "    Language = row.iloc[5,0]\n",
    "    Received = row.iloc[6,0]\n",
    "    Accepted = row.iloc[7,0]\n",
    "    Volume = row.iloc[8,0]\n",
    "    Issue = row.iloc[9,0]\n",
    "    Part = row.iloc[10,0]\n",
    "    FirstPage = row.iloc[11,0]\n",
    "    LastPage = row.iloc[12,0]\n",
    "    Comments = row.iloc[13,0]\n",
    "    CiteCount = row.iloc[14,0]\n",
    "    Note = row.iloc[15,0]\n",
    "    \n",
    "    # create a database connection\n",
    "    conn = create_connection(\"root_read.db\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO Article(Journal,PubDate,Title,Abstract, \\\n",
    "                Language, Received,  Accepted, Volume, Issue, Part, \\\n",
    "                FirstPage,  LastPage, Comments, CiteCount, Note) VALUES \\\n",
    "                (?,  ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "          (Journal,PubDate,Title,Abstract, Language, Received, Accepted, \\\n",
    "           Volume, Issue, Part, FirstPage, LastPage, Comments, CiteCount, Note))\n",
    "    conn.commit() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37ffb1499d5438daa3ef80b9291526c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b38f3c47474e42a01abbd98146e5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Update Author Table by Dynamically Inserting into a Database\n",
    "for row in tqdm_notebook(df.iterrows()):\n",
    "    \n",
    "    row = pd.DataFrame(row)\n",
    "    row = pd.DataFrame(row.iloc[1,0])\n",
    "    AuthorID = row.iloc[0,0]\n",
    "    AuthorName = row.iloc[1,0]\n",
    "    Sex = row.iloc[2,0]\n",
    "    NativeLanguage = \"MISSING\"\n",
    "    PhDYear = \"MISSING\"\n",
    "    Note = \"MISSING\"\n",
    "    \n",
    "    conn = create_connection(\"root_read.db\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT OR IGNORE INTO Author(AuthorID,AuthorName,Sex,NativeLanguage,PhDYear, Note) VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "          (AuthorID, AuthorName, Sex, NativeLanguage, PhDYear, Note))\n",
    "    conn.commit() \n",
    "\n",
    "# Load data that we just built  \n",
    "db = sqlite3.connect(\"root_read.db\")\n",
    "cursor = db.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "authors = pd.read_sql_query(\"SELECT * from Author\", db)\n",
    "articles = pd.read_sql_query(\"SELECT * from Article\", db)\n",
    "cursor.close()\n",
    "db.close()\n",
    "\n",
    "# Build the new AuthorCorr connections \n",
    "# Note: This needs to be done *after* authors and journals are first upodated\n",
    "# This cell is making the link between thm using merging and general data maniupaltion\n",
    "author_corr = pd.read_csv(\"AuthorCorr_temp.csv\")\n",
    "author_corr = author_corr[[\"confirmed name\",\"ArticleID\"]]\n",
    "author_corr.columns = [\"AuthorName\",\"ArticleID\"]\n",
    "author_corr = author_corr.merge(authors, on = \"AuthorName\")\n",
    "author_corr = author_corr[[\"AuthorID\", \"ArticleID\"]]\n",
    "author_corr = author_corr.sort_values(by = [\"ArticleID\"])\n",
    "author_corr['AuthorID'] = author_corr['AuthorID'].astype(str)\n",
    "author_corr['ArticleID'] = author_corr['ArticleID'].astype(str)\n",
    "AuthorCorr['AuthorID'] = AuthorCorr['AuthorID'].astype(str)\n",
    "AuthorCorr['ArticleID'] = AuthorCorr['ArticleID'].astype(str)\n",
    "author_corr = author_corr.drop_duplicates()\n",
    "\n",
    "# Update AuthorCorr Table by Dynamically Inserting into a Database\n",
    "for row in tqdm_notebook(author_corr.iterrows()):\n",
    "    \n",
    "    row = pd.DataFrame(row)\n",
    "    row = pd.DataFrame(row.iloc[1,0])\n",
    "    AuthorID = row.iloc[0,0]\n",
    "    ArticleID = row.iloc[1,0]\n",
    "    AuthorOrder = \"MISSING\"\n",
    "    conn = create_connection(\"root_read.db\")\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        # Note the IGNORE part of this statement, which helps us gracefully ignore the conflicts (WHY?)\n",
    "        # This will successfully insert any rows that don't have any violations\n",
    "        cur.execute(\"INSERT OR IGNORE INTO AuthorCorr(AuthorID,ArticleID,AuthorOrder) VALUES (?, ?, ?)\", (AuthorID, ArticleID, AuthorOrder))\n",
    "        conn.commit() \n",
    "    except Error as e:\n",
    "        print(\"skipped duplicate entry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Manual cleaning \n",
    "\n",
    "# manual_clean = new_authors.merge(new_articles, on = \"ArticleID\")\n",
    "#manual_clean = manual_clean[[\"metadata\",\"ArticleID\",\"Journal\", \"PubDate\", \"Title\"]]\n",
    "#manual_clean = manual_clean.drop(columns = [\"ArticleID\"])\n",
    "#manual_clean = manual_clean.drop_duplicates(subset=\"metadata\", keep='first', inplace=False)\n",
    "#master_key = pd.read_csv(\"key.csv\", header=None)\n",
    "#master_key.columns = [\"metadata\", \"clean name\", \"confirmed name\", \"Sex\"]\n",
    "#manual_clean= manual_clean.merge(master_key, on = \"metadata\", how = \"outer\")\n",
    "#manual_clean = manual_clean.replace(np.nan, '', regex=True)\n",
    "#manual_clean = manual_clean[manual_clean[\"Sex\"] != \"Male\" ]\n",
    "#manual_clean = manual_clean[manual_clean[\"Sex\"] != \"Female\" ] # technical debt!\n",
    "#manual_clean = new_authors.merge(new_articles, on = \"ArticleID\")\n",
    "#manual_clean = manual_clean[[\"metadata\",\"ArticleID\",\"Journal\", \"PubDate\", \"Title\"]]\n",
    "#manual_clean = manual_clean.drop(columns = [\"ArticleID\"])\n",
    "#manual_clean = manual_clean.drop_duplicates(subset=\"metadata\", keep='first', inplace=False)\n",
    "#master_key = pd.read_csv(\"masterkey.csv\", header=None)\n",
    "#master_key.columns = [\"metadata\", \"clean name\", \"confirmed name\", \"Sex\"]\n",
    "#manual_clean= manual_clean.merge(master_key, on = \"metadata\", how = \"outer\")\n",
    "##manual_clean = manual_clean.replace(np.nan, '', regex=True)\n",
    "#manual_clean = manual_clean[manual_clean[\"Sex\"] != \"Male\" ]\n",
    "#manual_clean = manual_clean[manual_clean[\"Sex\"] != \"Female\" ]\n",
    "#for i in tqdm_notebook(range(0, len(manual_clean))):\n",
    "#    string = manual_clean.iloc[i,0]\n",
    "#    string =  string.title()\n",
    "#    manual_clean.iloc[i,0] = string\n",
    "#manual_clean['metadata'] = manual_clean['metadata'].astype('str') #where the max length is set at 80 bytes,\n",
    "#manual_clean[\"last name\"] = \" \"\n",
    "#manual_clean[\"first name\"] = \" \"\n",
    "#for i in tqdm_notebook(range(0, len(manual_clean))):\n",
    "#    string = manual_clean.iloc[i,0]\n",
    "#    last = string.split(\",\")[0]\n",
    "#    try:\n",
    "#        first = string.split(\",\")[1]\n",
    "#    except IndexError:\n",
    "#        first = \"MISSING\"\n",
    "#    manual_clean.iloc[i,-1] = first\n",
    "#    manual_clean.iloc[i,-2] = last\n",
    "#manual_clean = manual_clean.reset_index(drop=True)\n",
    "#manual_clean[\"clean name\"] = manual_clean[\"first name\"]  + \" \" + manual_clean[\"last name\"] \n",
    "#manual_clean[\"first name\"] = manual_clean[\"first name\"].str.replace(' ', '') # tech debt!\n",
    "#manual_clean[\"first name\"] = manual_clean[\"first name\"].str.replace(' ', '')\n",
    "#manual_clean[\"first name\"] = manual_clean[\"first name\"].str.replace(' ', '')\n",
    "#manual_clean[\"first name\"] = manual_clean['first name'].astype(str).str[0]\n",
    "#manual_clean[\"name\"] = manual_clean[\"first name\"] + \" \"+ manual_clean[\"last name\"] \n",
    "#master_key[\"first name\"] = master_key['clean name'].astype(str).str[0]\n",
    "#master_key['last name'] = master_key['clean name'].str.split().str[-1]#master_key[\"last name\"] = master_key[\"last name\"][0]\\\n",
    "#master_key[\"name\"] = master_key[\"first name\"] + \" \"+ master_key[\"last name\"] \n",
    "#Author[\"first name\"] = Author['AuthorName'].astype(str).str[0]\n",
    "#Author['last name'] = Author['AuthorName'].str.split().str[-1]#master_key[\"last name\"] = master_key[\"last name\"][0]\\\n",
    "#Author[\"name\"] = Author[\"first name\"] + \" \" + Author[\"last name\"] \n",
    "#manual_clean[\"first name\"] = manual_clean['first name'].astype(str).str[1]\n",
    "#manual_clean[\"name\"] = manual_clean[\"first name\"] + \" \" + manual_clean[\"last name\"] \n",
    "#manual_clean_2 = manual_clean.merge(Author, on = [\"name\" ], )\n",
    "#manual_clean_2 = manual_clean_2.sort_values(by = \"clean name\")manual_clean_2.to_csv(\"manual_clean_2\")\n",
    "#Author = Author[[\"AuthorName\",\"Sex\"]]\n",
    "#Author[\"last name\"] = Author[\"AuthorName\"].str.split().str[-1]\n",
    "#Author[\"middle name\"] = Author[\"AuthorName\"].str.split().str[1]\n",
    "#Author[\"first name\"] = Author[\"AuthorName\"].str.split().str[0]\n",
    "#old_surnames = set(Author[\"last name\"])\n",
    "#matches = pd.DataFrame([])\n",
    "#for i in tqdm_notebook(range(0, len(new_authors))):\n",
    "#    \n",
    "#    last = new_authors.iloc[i,1]\n",
    "#    first = str(new_authors.iloc[i,-1])\n",
    "#    first = first.replace(' ', '')\n",
    "#    first = first.replace(' ', '')\n",
    "#    first_letter = first[0] # terrible naming conventions (technical debt!)\n",
    "#    \n",
    "#    match = Author[Author[\"last name\"] == last]\n",
    "#    if len(match)>1:\n",
    "#        first_name = first[:1]\n",
    "#        match[\"first name\"] = match[\"first name\"].astype(str).str[0]\n",
    "#        match = match[match[\"first name\"] == first_letter]\n",
    "#    if len(match == 1):\n",
    "#        match = match.iloc[:,0:2]\n",
    "#        \n",
    "#    if len(match)>1:\n",
    "#        match = pd.DataFrame([\" \", \" \"])\n",
    "#    if len(match)<1:\n",
    "#3        match = pd.DataFrame([\" \", \" \"])\n",
    "#\n",
    "#    match[\"idx\"] = i\n",
    "#    matches = matches.append(match)\n",
    "#matches = matches[[\"idx\", \"AuthorName\", \"Sex\"]]\n",
    "#matches = matches[matches['AuthorName'].notnull()]\n",
    "#old_articles = Article[[\"index\", \"ArticleID\", \"Journal\", \"Title\",\"PubDate\"]]\n",
    "#df = old_articles.append(new_articles)\n",
    "#df.to_csv(\"df.csv\")\n",
    "#new_authors.to_csv(\"authors_new.csv\") # backwards naming convention\n",
    "#new_authors[\"idx\"] = new_authors.index\n",
    "#new_authors = new_authors.merge(matches,on =\"idx\", how = \"outer\")\n",
    "#new_authors[\"Name\"] = new_authors[\"first name\"] + \" \" + new_authors[\"last name\"]\n",
    "#new_authors = new_authors[[\"metadata\",\"Name\",\"AuthorName\", \"Sex\"]]\n",
    "#new_authors = new_authors.dropna(subset=['metadata'])\n",
    "#new_authors.columns = [\"metadata\", \"Clean Name\", \"Possible Match\", \"Possible Sex\"]\n",
    "\n",
    "\n",
    "# Let's see what the data looks like. \n",
    "#manual_clean  = manual_clean.sort_values(by = \"clean name\")\n",
    "#print(manual_clean)\n",
    "# Print to .csv file so that you can manually tag the gender of each author\n",
    "#manual_clean_2.to_csv(\"manual_clean_5.csv\")\n",
    "\n",
    "\n",
    "#print(df)\n",
    "#df[\"journal\"] = df.index\n",
    "#df[\"journal\"] = df.iloc[df.index.get_level_values('journal') == 1]\n",
    "\n",
    "#df[\"journal\"] \n",
    "# Make \"new artice\" metadata complete \n",
    "# Fuzzy merge slightly different variations of the same name\n",
    "# Propoerly format this python code and notebook \n",
    "# Fix GitHuB\n",
    "# Append to data\n",
    "\n",
    "# Get rid of authors that are not already in dataset\n",
    "#existing_authors = set(Author.AuthorName)\n",
    "#new_authors = set(authors.metadata)\n",
    "#new_authors = [x for x in new_authors if not x in existing_authors]\n",
    "#new_authors = pd.DataFrame(new_authors)\n",
    "#new_authors[\"AuthorID\"] = range(0, len(new_authors))\n",
    "#new_authors[\"AuthorID\"] = new_authors[\"AuthorID\"] + Author.AuthorID.max()\n",
    "#new_authors.to_csv(\"new_authors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
